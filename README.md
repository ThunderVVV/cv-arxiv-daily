[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.08.31
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#mllm>MLLM</a></li>
  </ol>
</details>

## MLLM

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2024-08-29**|**PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning**|<div style="width:400px;">1. Mohamed Bin Zayed University of Artificial Intelligence</div>|[2408.16769](http://arxiv.org/abs/2408.16769)|**[link](https://github.com/nhussein/promptsmooth)**|
|**2024-08-29**|**VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation**|<div style="width:400px;">1. University of Science and Technology of China 2. Show Lab, National University of Singapore 3. Xiaohongshu.Inc 4. Institute for Infocomm Research, A*STAR</div>|[2408.16730](http://arxiv.org/abs/2408.16730)|null|
|**2024-08-29**|**Space3D-Bench: Spatial 3D Question Answering Benchmark**|<div style="width:400px;">1. ETH ZÃ¼rich 2. Microsoft</div>|[2408.16662](http://arxiv.org/abs/2408.16662)|null|
|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|<div style="width:400px;">1. Columbia University 2. Columbia University 3. Columbia University 4. Columbia University</div>|[2408.16647](http://arxiv.org/abs/2408.16647)|null|
|**2024-08-29**|**CogVLM2: Visual Language Models for Image and Video Understanding**|<div style="width:400px;">1. ZhipuAI 2. Tsinghua University</div>|[2408.16500](http://arxiv.org/abs/2408.16500)|**[link](https://github.com/thudm/cogvlm2)**|
|**2024-08-29**|**Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning**|<div style="width:400px;">1. Institute of Automation, Chinese Academy of Sciences 2. School of Artificial Intelligence, University of Chinese Academy of Sciences</div>|[2408.16486](http://arxiv.org/abs/2408.16486)|null|
|**2024-08-29**|**Text-Enhanced Zero-Shot Action Recognition: A training-free approach**|<div style="width:400px;">1. University of Trento 2. Fondazione Bruno Kessler (FBK) 3. University of Bologna</div>|[2408.16412](http://arxiv.org/abs/2408.16412)|null|
|**2024-08-29**|**Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models**|<div style="width:400px;">1. Kioxia Corporation</div>|[2408.16296](http://arxiv.org/abs/2408.16296)|null|
|**2024-08-29**|**Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation**|<div style="width:400px;">1. University of California, Berkeley</div>|[2408.16228](http://arxiv.org/abs/2408.16228)|null|
|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|<div style="width:400px;">1.Tsinghua University 2.Xiaomi AI Lab</div>|[2408.16224](http://arxiv.org/abs/2408.16224)|null|
|**2024-08-29**|**Training-free Video Temporal Grounding using Large-scale Pre-trained Models**|<div style="width:400px;">1. Wangxuan Institute of Computer Technology, Peking University 2. National Institute of Health Data Science, Peking University 3. State Key Laboratory of General Artificial Intelligence, Peking University</div>|[2408.16219](http://arxiv.org/abs/2408.16219)|null|
|**2024-08-28**|**VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images**|<div style="width:400px;">1.VirginiaTech 2.Univ.ofCalifornia,Irvine 3.UNCatChapelHill 4.TulaneUniv.</div>|[2408.16176](http://arxiv.org/abs/2408.16176)|**[link](https://github.com/sammarfy/vlm4bio)**|
|**2024-08-29**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|<div style="width:400px;">1.NationalTsingHuaUniversity 2.NVIDIA</div>|[2408.15996](http://arxiv.org/abs/2408.15996)|null|
|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|<div style="width:400px;">1. Technical University Dresden 2. StratifAI GmbH 3. German Cancer Research Center 4. Johannes Kepler University</div>|[2408.15823](http://arxiv.org/abs/2408.15823)|null|
|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|<div style="width:400px;">1. German Cancer Research Center (DKFZ)</div>|[2408.15802](http://arxiv.org/abs/2408.15802)|null|
|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**|<div style="width:400px;">1. Qilu University of Technology (Shandong Academy of Sciences) 2. Fuzhou University 3. Shanghai Jiao Tong University 4. Tongji University</div>|[2408.15740](http://arxiv.org/abs/2408.15740)|**[link](https://github.com/CV4RA/MambaPlace)**|
|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|<div style="width:400px;">1. MarkantServicesInternationalGmbH 2. OffenburgUniversity 3. InstituteforMachineLearningandAnalytics(IMLA)</div>|[2408.15626](http://arxiv.org/abs/2408.15626)|null|
|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|<div style="width:400px;">1. Fudan University 2. Shanghai Engineering Research Center of AI & Robotics 3. Shanghai Key Lab of Intelligent Information Processing 4. Academy for Engineering and Technology</div>|[2408.15566](http://arxiv.org/abs/2408.15566)|**[link](https://github.com/Jarvisgivemeasuit/tagood)**|
|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|<div style="width:400px;">1.Meituan Group 2.University of Chinese Academy of Sciences 3.Institute of Software, Chinese Academy of Sciences 4.Key Laboratory of System Software</div>|[2408.15542](http://arxiv.org/abs/2408.15542)|null|
|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|<div style="width:400px;">1.AIGraduateSchool,GIST,SouthKorea 2.NAVERCloud,SouthKorea 3.ElectricalEngineeringandComputerScience,GIST,SouthKorea</div>|[2408.15521](http://arxiv.org/abs/2408.15521)|null|
|**2024-08-28**|**Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models**|<div style="width:400px;">1.NexaAI</div>|[2408.15518](http://arxiv.org/abs/2408.15518)|null|
|**2024-08-28**|**AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models**|<div style="width:400px;">1. Aerospace Information Research Institute, Chinese Academy of Sciences 2. University of Chinese Academy of Sciences 3. School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences 4. Key Laboratory of Network Information System Technology (NIST)</div>|[2408.15511](http://arxiv.org/abs/2408.15511)|null|

<p align=right>(<a href=#updated-on-20240831>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues


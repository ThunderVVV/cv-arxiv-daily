[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.09.26
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#mllm>MLLM</a></li>
  </ol>
</details>

## MLLM

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2024-09-25**|**Attention Prompting on Image for Large Vision-Language Models**|1. National University of <br>Singapore|[2409.17143](http://arxiv.org/abs/2409.17143)|**[link](https://github.com/yu-rp/apiprompting)**|
|**2024-09-25**|**Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset**|1.UCBerkeley <br>2.CornellUniversity|[2409.17126](http://arxiv.org/abs/2409.17126)|null|
|**2024-09-25**|**Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?**|1.Groundlight|[2409.17080](http://arxiv.org/abs/2409.17080)|null|
|**2024-09-25**|**GeoBiked: A Dataset with Geometric Features and Automated Labeling Techniques to Enable Deep Generative Models in Engineering Design**|1.BMWGroup <br>2.UniversityofAugsburg|[2409.17045](http://arxiv.org/abs/2409.17045)|null|
|**2024-09-25**|**Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification**|1.TheUniversityofTokyo,2.Unive <br>rsityofSouthernCalifornia,3.RIKENCenterforAdvancedIntelligenceProject,4.TheChineseUniversityofHongKong|[2409.16718](http://arxiv.org/abs/2409.16718)|**[link](https://github.com/minglllli/clipfit)**|
|**2024-09-25**|**Semi-LLIE: Semi-supervised Contrastive Learning with Mamba-based Low-light Image Enhancement**|1. Northwestern Polytechnical <br>University 2. Science and <br>Technology on <br>Electromechanical Dynamic <br>Control Laboratory 3. School <br>of Astronautics 4. Key <br>Laboratory of Intelligent <br>Interaction and Applications|[2409.16604](http://arxiv.org/abs/2409.16604)|null|
|**2024-09-24**|**A Unified Hallucination Mitigation Framework for Large Vision-Language Models**|1.The University of Texas at <br>Dallas|[2409.16494](http://arxiv.org/abs/2409.16494)|null|
|**2024-09-24**|**BehAV: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes**|1. University of Maryland|[2409.16484](http://arxiv.org/abs/2409.16484)|null|
|**2024-09-24**|**Semantic Refocused Tuning for Open-Vocabulary Panoptic Segmentation**|1.TsinghuaUniversity <br>2.BoschCorporateResearch|[2409.16278](http://arxiv.org/abs/2409.16278)|null|
|**2024-09-24**|**Expert-level vision-language foundation model for real-world radiology and comprehensive evaluation**|1.Sun Yat-Sen University <br>2.Beijing University of Posts <br>and Telecommunications <br>3.Peking University|[2409.16183](http://arxiv.org/abs/2409.16183)|null|
|**2024-09-24**|**ComiCap: A VLMs pipeline for dense captioning of Comic Panels**|1. Computer Vision Center, UAB <br>2. University of Florence 3. <br>MICC 4. University of Florence|[2409.16159](http://arxiv.org/abs/2409.16159)|**[link](https://github.com/emanuelevivoli/comicap)**|
|**2024-09-24**|**Bridging Environments and Language with Rendering Functions and Vision-Language Models**|1. NAVER LABS Europe 2. <br>Institute of Intelligent <br>Systems and Robotics, Sorbonne <br>University|[2409.16024](http://arxiv.org/abs/2409.16024)|null|
|**2024-09-24**|**Overcoming Reward Model Noise in Instruction-Guided Reinforcement Learning**|1.TheUniversityofMelbourne|[2409.15922](http://arxiv.org/abs/2409.15922)|null|
|**2024-09-24**|**ReLEP: A Novel Framework for Real-world Long-horizon Embodied Planning**|1.PekingUniversity 2.|[2409.15658](http://arxiv.org/abs/2409.15658)|null|
|**2024-09-23**|**Clinical-grade Multi-Organ Pathology Report Generation for Multi-scale Whole Slide Images via a Semantically Guided Medical Text Foundation Model**|1. Korea University 2. The <br>Catholic University of Korea <br>3. Korea University|[2409.15574](http://arxiv.org/abs/2409.15574)|null|
|**2024-09-23**|**Discovering Object Attributes by Prompting Large Language Models with Perception-Action APIs**|1. University of Maryland|[2409.15505](http://arxiv.org/abs/2409.15505)|null|
|**2024-09-23**|**VLMine: Long-Tail Data Mining with Vision Language Models**|1.CruiseLLC 2.MetaInc.|[2409.15486](http://arxiv.org/abs/2409.15486)|null|
|**2024-09-23**|**Behavioral Bias of Vision-Language Models: A Behavioral Finance View**|1. University of Southern <br>California|[2409.15256](http://arxiv.org/abs/2409.15256)|null|
|**2024-09-23**|**ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models**|1.INSAIT, Sofia University|[2409.15250](http://arxiv.org/abs/2409.15250)|null|
|**2024-09-18**|**Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution**|1.QwenTeamAlibabaGroup|[2409.12191](http://arxiv.org/abs/2409.12191)|**[link](https://github.com/qwenlm/qwen2-vl)**|
|**2024-09-18**|**Mixture of Prompt Learning for Vision Language Models**|1.Tsinghua University|[2409.12011](http://arxiv.org/abs/2409.12011)|null|
|**2024-09-18**|**GauTOAO: Gaussian-based Task-Oriented Affordance of Objects**|1.Peking University|[2409.11941](http://arxiv.org/abs/2409.11941)|null|
|**2024-09-18**|**LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models**|1.Valeo.ai 2.Univ. Grenoble <br>Alpes 3.Sorbonne Université|[2409.11919](http://arxiv.org/abs/2409.11919)|null|
|**2024-09-18**|**VL-Reader: Vision and Language Reconstructor is an Effective Scene Text Recognizer**|1.AlibabaGroup <br>2.HuazhongUniversityofSciencea <br>ndTechnology 3.AlibabaGroup <br>4.HuazhongUniversityofSciencea <br>ndTechnology|[2409.11656](http://arxiv.org/abs/2409.11656)|null|
|**2024-09-17**|**Mamba Fusion: Learning Actions Through Questioning**|1. <br>GeorgiaInstituteofTechnology <br>2. StonyBrookUniversity 3. <br>GoogleResearch|[2409.11513](http://arxiv.org/abs/2409.11513)|**[link](https://github.com/dongzhikang/mambavl)**|
|**2024-09-17**|**NVLM: Open Frontier-Class Multimodal LLMs**|1.NVIDIA|[2409.11402](http://arxiv.org/abs/2409.11402)|null|
|**2024-09-17**|**CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark**|1.Princeton University|[2409.11363](http://arxiv.org/abs/2409.11363)|**[link](https://github.com/siegelz/core-bench)**|
|**2024-09-17**|**LPT++: Efficient Training on Mixture of Long-tailed Experts**|1. School of Computer Science <br>and Technology, Harbin <br>Institute of Technology 2. <br>School of Computing and <br>Information Systems, Singapore <br>Management University 3. The <br>Hong Kong Polytechnic <br>University|[2409.11323](http://arxiv.org/abs/2409.11323)|null|
|**2024-09-17**|**Improving the Efficiency of Visually Augmented Language Models**|1.UniversityoftheBasqueCountry <br>UPV/EHU|[2409.11148](http://arxiv.org/abs/2409.11148)|null|
|**2024-09-17**|**CAST: Cross-modal Alignment Similarity Test for Vision Language Models**|1. University of Edinburgh 2. <br>University of Trento|[2409.11007](http://arxiv.org/abs/2409.11007)|**[link](https://github.com/gautierdag/cast)**|
|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|1.TheUniversityofMelbourne|[2409.10921](http://arxiv.org/abs/2409.10921)|**[link](https://github.com/yanbei-jiang/artwork-interpretation)**|
|**2024-09-16**|**Benchmarking VLMs' Reasoning About Persuasive Atypical Images**| |[2409.10719](http://arxiv.org/abs/2409.10719)|null|
|**2024-09-16**|**MotIF: Motion Instruction Fine-tuning**|1. Massachusetts Institute of <br>Technology 2. Stanford <br>University 3. Carnegie Mellon <br>University|[2409.10683](http://arxiv.org/abs/2409.10683)|null|
|**2024-09-16**|**Do Pre-trained Vision-Language Models Encode Object States?**|1.BrownUniversity|[2409.10488](http://arxiv.org/abs/2409.10488)|null|
|**2024-09-16**|**CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera**|1. University of <br>CaliforniaSanDiego 2. <br>IntelLabs|[2409.10441](http://arxiv.org/abs/2409.10441)|null|
|**2024-09-16**|**HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models**|1.NewYorkUniversity|[2409.10419](http://arxiv.org/abs/2409.10419)|null|
|**2024-09-16**|**ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions**|1.Purdue University|[2409.10283](http://arxiv.org/abs/2409.10283)|null|
|**2024-09-16**|**NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions**|1. Monash University 2. Monash <br>University 3. Monash <br>University 4. Monash <br>University|[2409.10196](http://arxiv.org/abs/2409.10196)|null|
|**2024-09-16**|**MotionCom: Automatic and Motion-Aware Image Composition with LLM and Video Diffusion Prior**|1.NanyangTechnologicalUniversi <br>ty 2.AlibabaGroup|[2409.10090](http://arxiv.org/abs/2409.10090)|**[link](https://github.com/weijing-tao/MotionCom)**|
|**2024-09-05**|**Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding**|1. University of Illinois <br>Urbana-Champaign 2. Carnegie <br>Mellon University|[2409.03757](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|
|**2024-09-05**|**Text-Guided Mixup Towards Long-Tailed Image Categorization**|1. University of Washington 2. <br>Alibaba Group|[2409.03583](http://arxiv.org/abs/2409.03583)|**[link](https://github.com/rsamf/text-guided-mixup)**|
|**2024-09-05**|**FrozenSeg: Harmonizing Frozen Foundation Models for Open-Vocabulary Segmentation**|1. Harbin Institute of <br>Technology 2. University of <br>Surrey 3. Nanyang <br>Technological University|[2409.03525](http://arxiv.org/abs/2409.03525)|null|
|**2024-09-05**|**Have Large Vision-Language Models Mastered Art History?**|1.KULeuven 2.TUDelft|[2409.03521](http://arxiv.org/abs/2409.03521)|null|
|**2024-09-05**|**OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving**|1.FudanUniversity <br>2.TsinghuaUniversity|[2409.03272](http://arxiv.org/abs/2409.03272)|null|
|**2024-09-03**|**Multi-Modal Adapter for Vision-Language Models**|1. University of Amsterdam 2. <br>Vinted|[2409.02958](http://arxiv.org/abs/2409.02958)|**[link](https://github.com/dqmis/clip-mma)**|
|**2024-09-04**|**Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving**|1. ShanghaiTech University 2. <br>The Chinese University of Hong <br>Kong|[2409.02914](http://arxiv.org/abs/2409.02914)|null|
|**2024-09-04**|**Benchmarking Spurious Bias in Few-Shot Image Classifiers**|1. University of Virginia|[2409.02882](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|
|**2024-09-04**|**Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection**|1. Shenzhen University 2. <br>Tencent Youtu Lab|[2409.02664](http://arxiv.org/abs/2409.02664)|null|
|**2024-09-04**|**Vision-Language Navigation with Continual Learning**|1. State Key Laboratory of <br>Multimodal Artificial <br>Intelligence Systems 2. <br>Institute of Automation, <br>Chinese Academy of Sciences <br>(CASIA) 3. University of <br>Chinese Academy of Sciences <br>(UCAS)|[2409.02561](http://arxiv.org/abs/2409.02561)|null|
|**2024-09-04**|**Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments**|1. State Key Laboratory of <br>Multimodal Artificial <br>Intelligence Systems, <br>Institute of Automation, <br>Chinese Academy of Science <br>(CASIA), Beijing 2. University <br>of Chinese Academy of Sciences <br>(UCAS), Beijing 3. Department <br>of Computer Science, The <br>University of Hong Kong, Hong <br>Kong Special Administrative <br>Region (SAR)|[2409.02522](http://arxiv.org/abs/2409.02522)|null|
|**2024-09-04**|**MOSMOS: Multi-organ segmentation facilitated by medical report supervision**|1.Fudan University 2.The Hong <br>Kong University of Science and <br>Technology 3.Second Military <br>Medical University (Naval <br>Medical University) 4.Shanghai <br>Collaborative Innovation <br>Center of Intelligent Visual <br>Computing|[2409.02418](http://arxiv.org/abs/2409.02418)|null|
|**2024-09-04**|**Multi-modal Situated Reasoning in 3D Scenes**|1. State Key Laboratory of <br>General Artificial <br>Intelligence, BIGAI 2. Peking <br>University|[2409.02389](http://arxiv.org/abs/2409.02389)|null|
|**2024-09-03**|**Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems**|1. Indian Institute of <br>Technology Kanpur 2. New York <br>University 3. Indian Institute <br>of Technology Kanpur 4. Indian <br>Institute of Technology Kanpur|[2409.02278](http://arxiv.org/abs/2409.02278)|null|
|**2024-09-03**|**How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?**|1.AutodeskAIResearch|[2409.02253](http://arxiv.org/abs/2409.02253)|null|
|**2024-09-03**|**Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models**|1.The Chinese University of <br>Hong Kong 2.Shanghai <br>Artificial Intelligence <br>Laboratory|[2409.02101](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|
|**2024-09-03**|**GraspSplats: Efficient Manipulation with 3D Feature Splatting**|1. UCSanDiego|[2409.02084](http://arxiv.org/abs/2409.02084)|null|
|**2024-09-03**|**Boosting Vision-Language Models for Histopathology Classification: Predict all at once**|1. Université Catholique de <br>Louvain (UCLouvain) 2. <br>Université de Mons (UMons) 3. <br>École de Technologie <br>Supérieure (ÉTS) 4. Centre de <br>Recherche du Centre <br>Hospitalier de l’Université de <br>Montréal (CRCHUM)|[2409.01883](http://arxiv.org/abs/2409.01883)|**[link](https://github.com/fereshteshakeri/histo-transclip)**|
|**2024-09-03**|**Towards Generative Class Prompt Learning for Few-shot Visual Recognition**|1. University of North <br>Carolina at Chapel Hill 2. <br>Computer Vision Center & <br>Computer Science Department, <br>Universitat Autònoma de <br>Barcelona 3. MICC, University <br>of Florence 4. University of <br>Florence|[2409.01835](http://arxiv.org/abs/2409.01835)|null|
|**2024-09-04**|**When Does Visual Prompting Outperform Linear Probing for Vision-Language Models? A Likelihood Perspective**|1. National Tsing Hua <br>University 2. Dartmouth <br>College 3. IBM Research 4. The <br>Chinese University of Hong <br>Kong|[2409.01821](http://arxiv.org/abs/2409.01821)|**[link](https://github.com/ibm/vp-llr)**|
|**2024-08-29**|**PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning**|1. Mohamed Bin Zayed <br>University of Artificial <br>Intelligence|[2408.16769](http://arxiv.org/abs/2408.16769)|**[link](https://github.com/nhussein/promptsmooth)**|
|**2024-08-29**|**VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation**|1. University of Science and <br>Technology of China 2. Show <br>Lab, National University of <br>Singapore 3. Xiaohongshu.Inc <br>4. Institute for Infocomm <br>Research, A*STAR|[2408.16730](http://arxiv.org/abs/2408.16730)|null|
|**2024-08-29**|**Space3D-Bench: Spatial 3D Question Answering Benchmark**|1. ETH Zürich 2. Microsoft|[2408.16662](http://arxiv.org/abs/2408.16662)|null|
|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|1. Columbia University 2. <br>Columbia University 3. <br>Columbia University 4. <br>Columbia University|[2408.16647](http://arxiv.org/abs/2408.16647)|null|
|**2024-08-29**|**CogVLM2: Visual Language Models for Image and Video Understanding**|1. ZhipuAI 2. Tsinghua <br>University|[2408.16500](http://arxiv.org/abs/2408.16500)|**[link](https://github.com/thudm/cogvlm2)**|
|**2024-08-29**|**Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning**|1. Institute of Automation, <br>Chinese Academy of Sciences 2. <br>School of Artificial <br>Intelligence, University of <br>Chinese Academy of Sciences|[2408.16486](http://arxiv.org/abs/2408.16486)|null|
|**2024-08-29**|**Text-Enhanced Zero-Shot Action Recognition: A training-free approach**|1. University of Trento 2. <br>Fondazione Bruno Kessler (FBK) <br>3. University of Bologna|[2408.16412](http://arxiv.org/abs/2408.16412)|null|
|**2024-08-29**|**Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models**|1. Kioxia Corporation|[2408.16296](http://arxiv.org/abs/2408.16296)|null|
|**2024-08-29**|**Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation**|1. University of California, <br>Berkeley|[2408.16228](http://arxiv.org/abs/2408.16228)|null|
|**2024-08-30**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|1.Tsinghua University 2.Xiaomi <br>AI Lab|[2408.16224](http://arxiv.org/abs/2408.16224)|null|
|**2024-08-29**|**Training-free Video Temporal Grounding using Large-scale Pre-trained Models**|1. Wangxuan Institute of <br>Computer Technology, Peking <br>University 2. National <br>Institute of Health Data <br>Science, Peking University 3. <br>State Key Laboratory of <br>General Artificial <br>Intelligence, Peking <br>University|[2408.16219](http://arxiv.org/abs/2408.16219)|null|
|**2024-08-28**|**VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images**|1.VirginiaTech <br>2.Univ.ofCalifornia,Irvine <br>3.UNCatChapelHill <br>4.TulaneUniv.|[2408.16176](http://arxiv.org/abs/2408.16176)|**[link](https://github.com/sammarfy/vlm4bio)**|
|**2024-08-29**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|1. National Tsing Hua <br>University 2. NVIDIA|[2408.15996](http://arxiv.org/abs/2408.15996)|null|
|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|1. Technical University <br>Dresden 2. StratifAI GmbH 3. <br>German Cancer Research Center <br>4. Johannes Kepler University|[2408.15823](http://arxiv.org/abs/2408.15823)|null|
|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|1. German Cancer Research <br>Center (DKFZ)|[2408.15802](http://arxiv.org/abs/2408.15802)|null|
|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**| |[2408.15740](http://arxiv.org/abs/2408.15740)|**[link](https://github.com/CV4RA/MambaPlace)**|
|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|1. <br>MarkantServicesInternationalGm <br>bH 2. OffenburgUniversity 3. <br>InstituteforMachineLearningand <br>Analytics(IMLA)|[2408.15626](http://arxiv.org/abs/2408.15626)|null|
|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|1. Fudan University 2. <br>Shanghai Engineering Research <br>Center of AI & Robotics 3. <br>Shanghai Key Lab of <br>Intelligent Information <br>Processing 4. Academy for <br>Engineering and Technology|[2408.15566](http://arxiv.org/abs/2408.15566)|**[link](https://github.com/Jarvisgivemeasuit/tagood)**|
|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|1.Meituan Group 2.University <br>of Chinese Academy of Sciences <br>3.Institute of Software, <br>Chinese Academy of Sciences <br>4.Key Laboratory of System <br>Software|[2408.15542](http://arxiv.org/abs/2408.15542)|null|
|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|1.GIST 2.NAVER Cloud <br>3.Electrical Engineering and <br>Computer Science|[2408.15521](http://arxiv.org/abs/2408.15521)|null|

<p align=right>(<a href=#updated-on-20240926>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues


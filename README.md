[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

## Updated on 2024.09.02
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#mllm>MLLM</a></li>
  </ol>
</details>

## MLLM

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2024-08-29**|**PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning**|1. Mohamed Bin Zayed <br>University of Artificial <br>Intelligence|[2408.16769](http://arxiv.org/abs/2408.16769)|**[link](https://github.com/nhussein/promptsmooth)**|
|**2024-08-29**|**VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation**|1. University of Science and <br>Technology of China 2. Show <br>Lab, National University of <br>Singapore 3. Xiaohongshu.Inc <br>4. Institute for Infocomm <br>Research, A*STAR|[2408.16730](http://arxiv.org/abs/2408.16730)|null|
|**2024-08-29**|**Space3D-Bench: Spatial 3D Question Answering Benchmark**|1. ETH ZÃ¼rich 2. Microsoft|[2408.16662](http://arxiv.org/abs/2408.16662)|null|
|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|1. Columbia University 2. <br>Columbia University 3. <br>Columbia University 4. <br>Columbia University|[2408.16647](http://arxiv.org/abs/2408.16647)|null|
|**2024-08-29**|**CogVLM2: Visual Language Models for Image and Video Understanding**|1. ZhipuAI 2. Tsinghua <br>University|[2408.16500](http://arxiv.org/abs/2408.16500)|**[link](https://github.com/thudm/cogvlm2)**|
|**2024-08-29**|**Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning**|1. Institute of Automation, <br>Chinese Academy of Sciences 2. <br>School of Artificial <br>Intelligence, University of <br>Chinese Academy of Sciences|[2408.16486](http://arxiv.org/abs/2408.16486)|null|
|**2024-08-29**|**Text-Enhanced Zero-Shot Action Recognition: A training-free approach**|1. University of Trento 2. <br>Fondazione Bruno Kessler (FBK) <br>3. University of Bologna|[2408.16412](http://arxiv.org/abs/2408.16412)|null|
|**2024-08-29**|**Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models**|1. Kioxia Corporation|[2408.16296](http://arxiv.org/abs/2408.16296)|null|
|**2024-08-29**|**Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation**|1. University of California, <br>Berkeley|[2408.16228](http://arxiv.org/abs/2408.16228)|null|
|**2024-08-30**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|1.Tsinghua University 2.Xiaomi <br>AI Lab|[2408.16224](http://arxiv.org/abs/2408.16224)|null|
|**2024-08-29**|**Training-free Video Temporal Grounding using Large-scale Pre-trained Models**|1. Wangxuan Institute of <br>Computer Technology, Peking <br>University 2. National <br>Institute of Health Data <br>Science, Peking University 3. <br>State Key Laboratory of <br>General Artificial <br>Intelligence, Peking <br>University|[2408.16219](http://arxiv.org/abs/2408.16219)|null|
|**2024-08-28**|**VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images**|1.VirginiaTech <br>2.Univ.ofCalifornia,Irvine <br>3.UNCatChapelHill <br>4.TulaneUniv.|[2408.16176](http://arxiv.org/abs/2408.16176)|**[link](https://github.com/sammarfy/vlm4bio)**|
|**2024-08-29**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|1. National Tsing Hua <br>University 2. NVIDIA|[2408.15996](http://arxiv.org/abs/2408.15996)|null|
|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|1. Technical University <br>Dresden 2. StratifAI GmbH 3. <br>German Cancer Research Center <br>4. Johannes Kepler University|[2408.15823](http://arxiv.org/abs/2408.15823)|null|
|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|1. German Cancer Research <br>Center (DKFZ)|[2408.15802](http://arxiv.org/abs/2408.15802)|null|
|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**| |[2408.15740](http://arxiv.org/abs/2408.15740)|**[link](https://github.com/CV4RA/MambaPlace)**|
|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|1. <br>MarkantServicesInternationalGm <br>bH 2. OffenburgUniversity 3. <br>InstituteforMachineLearningand <br>Analytics(IMLA)|[2408.15626](http://arxiv.org/abs/2408.15626)|null|
|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|1. Fudan University 2. <br>Shanghai Engineering Research <br>Center of AI & Robotics 3. <br>Shanghai Key Lab of <br>Intelligent Information <br>Processing 4. Academy for <br>Engineering and Technology|[2408.15566](http://arxiv.org/abs/2408.15566)|**[link](https://github.com/Jarvisgivemeasuit/tagood)**|
|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|1.Meituan Group 2.University <br>of Chinese Academy of Sciences <br>3.Institute of Software, <br>Chinese Academy of Sciences <br>4.Key Laboratory of System <br>Software|[2408.15542](http://arxiv.org/abs/2408.15542)|null|
|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|1.GIST 2.NAVER Cloud <br>3.Electrical Engineering and <br>Computer Science|[2408.15521](http://arxiv.org/abs/2408.15521)|null|

<p align=right>(<a href=#updated-on-20240902>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/Vincentqyw/cv-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/Vincentqyw/cv-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/Vincentqyw/cv-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/Vincentqyw/cv-arxiv-daily/issues


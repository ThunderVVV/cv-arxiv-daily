{"MLLM": {"2408.15996": "|**2024-08-29**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|1. National Tsing Hua Universi<br>ty 2. NVIDIA|[2408.15996](http://arxiv.org/abs/2408.15996)|null|\n", "2408.15823": "|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|1. Technical University Dresde<br>n 2. StratifAI GmbH 3. Univers<br>ity Hospital and Faculty of Me<br>dicine Carl Gustav Carus 4. Ge<br>rman Cancer Research Center|[2408.15823](http://arxiv.org/abs/2408.15823)|null|\n", "2408.15802": "|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|1. German Cancer Research Cent<br>er (DKFZ)|[2408.15802](http://arxiv.org/abs/2408.15802)|null|\n", "2408.15740": "|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**|1. Qilu University of Technolo<br>gy (Shandong Academy of Scienc<br>es) 2. Fuzhou University 3. Sh<br>anghai Jiao Tong University 4.<br> Tongji University|[2408.15740](http://arxiv.org/abs/2408.15740)|**[link](https://github.com/CV4RA/MambaPlace)**|\n", "2408.15626": "|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|1. MarkantServicesInternationa<br>lGmbH 2. OffenburgUniversity 3<br>. InstituteforMachineLearninga<br>ndAnalytics(IMLA)|[2408.15626](http://arxiv.org/abs/2408.15626)|null|\n", "2408.15566": "|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|1. Fudan University 2. Shangha<br>i Engineering Research Center <br>of AI & Robotics 3. Shanghai K<br>ey Lab of Intelligent Informat<br>ion Processing 4. Academy for <br>Engineering and Technology|[2408.15566](http://arxiv.org/abs/2408.15566)|**[link](https://github.com/Jarvisgivemeasuit/tagood)**|\n", "2408.15542": "|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|1.Meituan Group 2.University o<br>f Chinese Academy of Sciences <br>3.Institute of Software, Chine<br>se Academy of Sciences 4.Key L<br>aboratory of System Software|[2408.15542](http://arxiv.org/abs/2408.15542)|null|\n", "2408.15521": "|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|1.GIST 2.NAVER Cloud 3.Electri<br>cal Engineering and Computer S<br>cience|[2408.15521](http://arxiv.org/abs/2408.15521)|null|\n", "2408.16769": "|**2024-08-29**|**PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning**| |[2408.16769](http://arxiv.org/abs/2408.16769)|**[link](https://github.com/nhussein/promptsmooth)**|\n", "2408.16730": "|**2024-08-29**|**VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation**|1. University of Science and T<br>echnology of China 2. Show Lab<br>, National University of Singa<br>pore 3. Xiaohongshu.Inc 4. Ins<br>titute for Infocomm Research, <br>A*STAR|[2408.16730](http://arxiv.org/abs/2408.16730)|null|\n", "2408.16662": "|**2024-08-29**|**Space3D-Bench: Spatial 3D Question Answering Benchmark**|1. ETH Z\u00fcrich 2. Microsoft|[2408.16662](http://arxiv.org/abs/2408.16662)|null|\n", "2408.16647": "|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|1. Columbia University 2. Colu<br>mbia University 3. Columbia Un<br>iversity 4. Columbia Universit<br>y|[2408.16647](http://arxiv.org/abs/2408.16647)|null|\n", "2408.16500": "|**2024-08-29**|**CogVLM2: Visual Language Models for Image and Video Understanding**|1. ZhipuAI 2. Tsinghua Univers<br>ity|[2408.16500](http://arxiv.org/abs/2408.16500)|**[link](https://github.com/thudm/cogvlm2)**|\n", "2408.16486": "|**2024-08-29**|**Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning**|1. Institute of Automation, Ch<br>inese Academy of Sciences 2. S<br>chool of Artificial Intelligen<br>ce, University of Chinese Acad<br>emy of Sciences|[2408.16486](http://arxiv.org/abs/2408.16486)|null|\n", "2408.16412": "|**2024-08-29**|**Text-Enhanced Zero-Shot Action Recognition: A training-free approach**|1. University of Trento 2. Fon<br>dazione Bruno Kessler (FBK) 3.<br> University of Bologna|[2408.16412](http://arxiv.org/abs/2408.16412)|null|\n", "2408.16296": "|**2024-08-29**|**Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models**|1. Kioxia Corporation|[2408.16296](http://arxiv.org/abs/2408.16296)|null|\n", "2408.16228": "|**2024-08-29**|**Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation**|1. University of California, B<br>erkeley|[2408.16228](http://arxiv.org/abs/2408.16228)|null|\n", "2408.16224": "|**2024-08-29**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|1.Tsinghua University 2.Xiaomi<br> AI Lab|[2408.16224](http://arxiv.org/abs/2408.16224)|null|\n", "2408.16219": "|**2024-08-29**|**Training-free Video Temporal Grounding using Large-scale Pre-trained Models**|1. Wangxuan Institute of Compu<br>ter Technology, Peking Univers<br>ity 2. National Institute of H<br>ealth Data Science, Peking Uni<br>versity 3. State Key Laborator<br>y of General Artificial Intell<br>igence, Peking University|[2408.16219](http://arxiv.org/abs/2408.16219)|null|\n", "2408.16176": "|**2024-08-28**|**VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images**|1.VirginiaTech 2.Univ.ofCalifo<br>rnia,Irvine 3.UNCatChapelHill <br>4.TulaneUniv.|[2408.16176](http://arxiv.org/abs/2408.16176)|**[link](https://github.com/sammarfy/vlm4bio)**|\n"}}
{"MLLM": {"2408.15996": "|**2024-08-28**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|<div style=\"width:400px;\">1. National Tsing Hua University 2. NVIDIA</div>|[2408.15996](http://arxiv.org/abs/2408.15996)|null|\n", "2408.15823": "|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|<div style=\"width:400px;\">1. Technical University Dresden 2. StratifAI GmbH 3. University Hospital and Faculty of Medicine Carl Gustav Carus 4. German Cancer Research Center</div>|[2408.15823](http://arxiv.org/abs/2408.15823)|null|\n", "2408.15802": "|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|<div style=\"width:400px;\">1. German Cancer Research Center (DKFZ)</div>|[2408.15802](http://arxiv.org/abs/2408.15802)|null|\n", "2408.15740": "|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**|<div style=\"width:400px;\">1. Qilu University of Technology (Shandong Academy of Sciences) 2. Fuzhou University 3. Shanghai Jiao Tong University 4. Tongji University</div>|[2408.15740](http://arxiv.org/abs/2408.15740)|null|\n", "2408.15626": "|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|<div style=\"width:400px;\">1. MarkantServicesInternationalGmbH 2. OffenburgUniversity 3. InstituteforMachineLearningandAnalytics(IMLA)</div>|[2408.15626](http://arxiv.org/abs/2408.15626)|null|\n", "2408.15566": "|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|<div style=\"width:400px;\">1. Fudan University 2. Shanghai Engineering Research Center of AI & Robotics 3. Shanghai Key Lab of Intelligent Information Processing 4. School of Computer Science, Fudan University</div>|[2408.15566](http://arxiv.org/abs/2408.15566)|null|\n", "2408.15542": "|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|<div style=\"width:400px;\">1.Meituan Group 2.University of Chinese Academy of Sciences (UCAS) 3.Chinese Academy of Sciences 4.Meituan Group</div>|[2408.15542](http://arxiv.org/abs/2408.15542)|null|\n", "2408.15521": "|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|<div style=\"width:400px;\">1.GIST 2.NAVER Cloud 3.GIST</div>|[2408.15521](http://arxiv.org/abs/2408.15521)|null|\n", "2408.15518": "|**2024-08-28**|**Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models**|<div style=\"width:400px;\">1.NexaAI</div>|[2408.15518](http://arxiv.org/abs/2408.15518)|null|\n", "2408.15511": "|**2024-08-28**|**AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models**|<div style=\"width:400px;\">1. Aerospace Information Research Institute, Chinese Academy of Sciences 2. University of Chinese Academy of Sciences 3. School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences 4. Key Laboratory of Network Information System Technology (NIST)</div>|[2408.15511](http://arxiv.org/abs/2408.15511)|null|\n"}}
{"MLLM": {"2408.15128": "|**2024-08-27**|**Evaluating the Energy Consumption of Machine Learning: Systematic Literature Review and Experiments**|Charlotte Rodriguez et.al.1. AccentureLabs, Sophia Antipolis, France 2. Inria Universit\u00e9 C\u00f4te d'Azur, Sophia Antipolis, France|[2408.15128](http://arxiv.org/abs/2408.15128)|null|\n", "2408.14895": "|**2024-08-28**|**VHAKG: A Multi-modal Knowledge Graph Based on Synchronized Multi-view Videos of Daily Activities**|Shusaku Egami et.al.1. National Institute of Advanced Industrial Science and Technology (AIST), Kawasaki-shi, Kanagawa, Japan 2. Fujitsu Ltd., Koto-ku, Tokyo, Japan 3. National Institute of Advanced Industrial Science and Technology (AIST), Koto-ku, Tokyo, Japan 4. National Institute of Advanced Industrial Science and Technology (AIST), Koto-ku, Tokyo, Japan 5. ACM, New York, NY, USA|[2408.14895](http://arxiv.org/abs/2408.14895)|null|\n", "2408.14812": "|**2024-08-27**|**HPT++: Hierarchically Prompting Vision-Language Models with Multi-Granularity Knowledge Generation and Improved Structure Modeling**|Yubin Wang et.al.1.Department of Computer Science and Technology, Tongji University, Shanghai, China. 2.Microsoft Research Asia, Shanghai, China. 3.School of Telecommunications Engineering, Xidian University, Xi\u2019an, China.|[2408.14812](http://arxiv.org/abs/2408.14812)|null|\n", "2408.14776": "|**2024-08-27**|**MROVSeg: Breaking the Resolution Curse of Vision-Language Models in Open-Vocabulary Semantic Segmentation**|Yuanbing Zhu et.al.1. Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences 2. School of Artificial Intelligence, University of Chinese Academy of Sciences 3. Centre for Artificial Intelligence and Robotics (CAIR), HKISI-CAS|[2408.14776](http://arxiv.org/abs/2408.14776)|null|\n", "2408.14744": "|**2024-08-27**|**RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models**|Junyao Ge et.al.1. School of Electronic Engineering, Xidian University, Xi'an, Shaanxi 710071, China|[2408.14744](http://arxiv.org/abs/2408.14744)|**[link](https://github.com/slytheringe/rsteller)**|\n", "2408.14723": "|**2024-08-27**|**Snap and Diagnose: An Advanced Multimodal Retrieval System for Identifying Plant Diseases in the Wild**|Tianqi Wei et.al.1.The University of Queensland|[2408.14723](http://arxiv.org/abs/2408.14723)|null|\n", "2408.14435": "|**2024-08-26**|**Social perception of faces in a vision-language model**|Carina I. Hausladen et.al.1. California Institute of Technology 2. ETH Zurich, Computational Social Science 3. ETH Zurich, Swiss Data Science Center, Empa|[2408.14435](http://arxiv.org/abs/2408.14435)|**[link](https://github.com/carinahausladen/clip-face-bias)**|\n", "2408.14153": "|**2024-08-26**|**Explaining Vision-Language Similarities in Dual Encoders with Feature-Pair Attributions**|Lucas M\u00f6ller et.al.1.InstituteforNaturalLanguageProcessing,UniversityofStuttgart|[2408.14153](http://arxiv.org/abs/2408.14153)|null|\n", "2408.14032": "|**2024-08-26**|**More Pictures Say More: Visual Intersection Network for Open Set Object Detection**|Bingcheng Dong et.al.1.Dalian University of Technology|[2408.14032](http://arxiv.org/abs/2408.14032)|null|\n", "2408.13979": "|**2024-08-26**|**Nemesis: Normalizing the Soft-prompt Vectors of Vision-Language Models**|Shuai Fu et.al.1.Department of Computer Science and Engineering, Southern University of Science and Technology 2.Computer Science Research Centre, University of Surrey|[2408.13979](http://arxiv.org/abs/2408.13979)|**[link](https://github.com/shyfoo/nemesis)**|\n", "2408.13909": "|**2024-08-25**|**LowCLIP: Adapting the CLIP Model Architecture for Low-Resource Languages in Multimodal Image Retrieval Task**|Ali Asgarov et.al.1. George Washington University 2. ADA University|[2408.13909](http://arxiv.org/abs/2408.13909)|**[link](https://github.com/aliasgerovs/azclip)**|\n", "2408.13898": "|**2024-08-25**|**Evaluating Attribute Comprehension in Large Vision-Language Models**|Haiwen Zhang et.al.1.Beijing University of Posts and Telecommunications, Beijing, China 2.Beijing Academy of Artificial Intelligence, Beijing, China|[2408.13898](http://arxiv.org/abs/2408.13898)|**[link](https://github.com/zhwwwww/attribute-comprehension-of-vlms)**|\n", "2408.13461": "|**2024-08-24**|**Probing the Robustness of Vision-Language Pretrained Models: A Multimodal Adversarial Attack Approach**|Jiwei Guan et.al.1. School of Computing, Macquarie University, Sydney, Australia 2. Applied Sciences Group, Microsoft Corporation, Redmond, USA 3. School of Information Technology, Deakin University, Waurn Ponds, Australia 4. Data61, CSIRO, Sydney, Australia|[2408.13461](http://arxiv.org/abs/2408.13461)|null|\n", "2408.13438": "|**2024-08-24**|**Explainable Concept Generation through Vision-Language Preference Learning**|Aditya Taparia et.al.1. Arizona State University 2. Arizona State University 3. Arizona State University|[2408.13438](http://arxiv.org/abs/2408.13438)|null|\n", "2408.13320": "|**2024-08-23**|**Online Zero-Shot Classification with CLIP**|Qi Qian et.al.1.Alibaba Group, Bellevue, WA 98004, USA 2.School of Engineering and Technology, University of Washington, Tacoma, WA 98402, USA|[2408.13320](http://arxiv.org/abs/2408.13320)|**[link](https://github.com/idstcv/onzeta)**|\n", "2408.13248": "|**2024-08-23**|**Foundational Model for Electron Micrograph Analysis: Instruction-Tuning Small-Scale Language-and-Vision Assistant for Enterprise Adoption**|Sakhinana Sagar Srinivas et.al.1.TCS Research, Bangalore 2.IIT Dharwad 3.IIIT Pune|[2408.13248](http://arxiv.org/abs/2408.13248)|null|\n", "2408.14496": "|**2024-08-23**|**A New Era in Computational Pathology: A Survey on Foundation and Vision-Language Models**|Dibaloke Chanda et.al.The provided text does not contain explicit information about the affiliations of the authors. To extract affiliations, the text would need to include details such as university names, research institutions, or company affiliations associated with the authors' names. Since the text provided is an abstract and does not include such details, it is not possible to provide the requested affiliations based on the current information. If the full text of the paper is available and includes affiliation details, I could then extract them accordingly.|[2408.14496](http://arxiv.org/abs/2408.14496)|null|\n", "2408.12928": "|**2024-08-23**|**ParGo: Bridging Vision-Language with Partial and Global Views**|An-Lan Wang et.al.1.ByteDanceChina 2.SchoolofComputerScienceandEngineering,SunYat-senUniversity,China|[2408.12928](http://arxiv.org/abs/2408.12928)|null|\n", "2408.12902": "|**2024-08-23**|**IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities**|Bin Wang et.al.1. 360 AI Research|[2408.12902](http://arxiv.org/abs/2408.12902)|null|\n", "2408.12808": "|**2024-08-23**|**VALE: A Multimodal Visual and Language Explanation Framework for Image Classifiers using eXplainable AI and Language Models**|Purushothaman Natarajan et.al.1.SRM Institute of Science and Technology|[2408.12808](http://arxiv.org/abs/2408.12808)|null|\n", "2408.12800": "|**2024-08-23**|**Cap2Sum: Learning to Summarize Videos by Generating Captions**|Cairong Zhao et.al.1. Department of Computer Science, Tongji University 2. Department of Computer Science, Tongji University 3. Department of Computer Science, Tongji University 4. Oosto, Belfast 5. Alibaba Group|[2408.12800](http://arxiv.org/abs/2408.12800)|null|\n", "2408.12637": "|**2024-08-22**|**Building and better understanding vision-language models: insights and future directions**|Hugo Lauren\u00e7on et.al.1. HuggingFace|[2408.12637](http://arxiv.org/abs/2408.12637)|null|\n", "2408.12528": "|**2024-08-25**|**Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**|Jinheng Xie et.al.1.ShowLab,NationalUniversityofSingapore 2.ByteDance|[2408.12528](http://arxiv.org/abs/2408.12528)|null|\n", "2408.12317": "|**2024-08-22**|**Adapt CLIP as Aggregation Instructor for Image Dehazing**|Xiaozhe Zhang et.al.1. Beihang University 2. Beihang University 3. Beihang University 4. Beihang University 5. Beihang University|[2408.12317](http://arxiv.org/abs/2408.12317)|null|\n", "2408.12246": "|**2024-08-22**|**OVA-DETR: Open Vocabulary Aerial Object Detection Using Image-Text Alignment and Fusion**|Guoting Wei et.al.1. Nanjing University of Science and Technology 2. Zhejiang Lab 3. Northwestern Polytechnical University 4. Intellifusion 5. (Not provided, only 4 affiliations are mentioned)|[2408.12246](http://arxiv.org/abs/2408.12246)|null|\n", "2408.12141": "|**2024-08-22**|**TRRG: Towards Truthful Radiology Report Generation With Cross-modal Disease Clue Enhanced Large Language Model**|Yuhao Wang et.al.1.GreatBayUniversity 2.TheHongKongPolytechnicUniversity 3.ShenzhenUniversity 4.MacaoPolytechnicUniversity|[2408.12141](http://arxiv.org/abs/2408.12141)|null|\n", "2408.12114": "|**2024-08-23**|**SPARK: Multi-Vision Sensor Perception and Reasoning Benchmark for Large-scale Vision-Language Models**|Youngjoon Yu et.al.1. Integrated Vision Language Lab, KAIST, South Korea|[2408.12114](http://arxiv.org/abs/2408.12114)|**[link](https://github.com/top-yun/spark)**|\n", "2408.12109": "|**2024-08-22**|**RoVRM: A Robust Visual Reward Model Optimized via Auxiliary Textual Preference Data**|Chenglong Wang et.al.1. School of Computer Science and Engineering, Northeastern University, Shenyang, China 2. NiuTrans Research, Shenyang, China 3. CAS Key Laboratory of Behavioral Science, Institute of Psychology, CAS, Beijing, China|[2408.12109](http://arxiv.org/abs/2408.12109)|null|\n", "2408.11813": "|**2024-08-21**|**SEA: Supervised Embedding Alignment for Token-Level Visual-Textual Integration in MLLMs**|Yuanyang Yin et.al.1. University of Science and Technology of China 2. Peking University 3. Kuaishou Technology|[2408.11813](http://arxiv.org/abs/2408.11813)|null|\n", "2408.11748": "|**2024-08-22**|**DH-Bench: Probing Depth and Height Perception of Large Visual-Language Models**|Shehreen Azad et.al.1. Center for Research in Computer Vision, University of Central Florida 2. Microsoft Research 3. Indian Institute of Technology, Kharagpur|[2408.11748](http://arxiv.org/abs/2408.11748)|**[link](https://github.com/sacrcv/dh-bench)**|\n", "2408.11742": "|**2024-08-21**|**CluMo: Cluster-based Modality Fusion Prompt for Continual Learning in Visual Question Answering**|Yuliang Cai et.al.1. University of Southern California 2. University of Southern California|[2408.11742](http://arxiv.org/abs/2408.11742)|**[link](https://github.com/yuliangcai2022/clumo)**|\n", "2408.11505": "|**2024-08-21**|**MSCPT: Few-shot Whole Slide Image Classification with Multi-scale and Context-focused Prompt Tuning**|Minghao Han et.al.1. Academy for Engineering and Technology, Fudan University, Shanghai 200433, China 2. Digital Medical Research Center, School of Basic Medical Science, Fudan University, Shanghai 200032, China 3. Zhongshan Hospital, Fudan University, Shanghai 200032, China|[2408.11505](http://arxiv.org/abs/2408.11505)|null|\n", "2408.11449": "|**2024-08-21**|**Enabling Small Models for Zero-Shot Classification through Model Label Learning**|Jia Zhang et.al.1. National Key Laboratory for Novel Software Technology, Nanjing University 2. School of Artifical Intelligence, Nanjing University 3. School of Intelligence Science and Technology, Nanjing University|[2408.11449](http://arxiv.org/abs/2408.11449)|null|\n", "2408.11380": "|**2024-08-21**|**Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models**|Kento Kawaharazuka et.al.1.TheDepartmentofMechano-Informatics,GraduateSchoolofInformationScienceandTechnology,TheUniversityofTokyo|[2408.11380](http://arxiv.org/abs/2408.11380)|null|\n", "2408.11312": "|**2024-08-21**|**Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework**|Xiao Han et.al.1.CityUniversityofHongKong 2.UniversityofScienceandTechnologyofChina 3.CareerScienceLab|[2408.11312](http://arxiv.org/abs/2408.11312)|null|\n", "2408.11305": "|**2024-08-21**|**UniFashion: A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation**|Xiangyu Zhao et.al.1. Department of Computing, The Hong Kong Polytechnic University|[2408.11305](http://arxiv.org/abs/2408.11305)|**[link](https://github.com/xiangyu-mm/unifashion)**|\n", "2408.11297": "|**2024-08-21**|**Making Large Vision Language Models to be Good Few-shot Learners**|Fan Liu et.al.1. Hohai University 2. HKUST 3. Griffith University 4. (Not provided) 5. (Not provided)|[2408.11297](http://arxiv.org/abs/2408.11297)|null|\n", "2408.11261": "|**2024-08-21**|**Towards Analyzing and Mitigating Sycophancy in Large Vision-Language Models**|Yunpu Zhao et.al.|[2408.11261](http://arxiv.org/abs/2408.11261)|null|\n", "2408.10945": "|**2024-08-20**|**HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models in Resource-Constrained Environments**|Kazi Hasan Ibn Arif et.al.|[2408.10945](http://arxiv.org/abs/2408.10945)|**[link](https://github.com/hasanar1f/hired)**|\n", "2408.10894": "|**2024-08-20**|**ViLReF: A Chinese Vision-Language Retinal Foundation Model**|Shengzhu Yang et.al.|[2408.10894](http://arxiv.org/abs/2408.10894)|**[link](https://github.com/t6yang/vilref)**|\n", "2408.10872": "|**2024-08-21**|**V-RoAst: A New Dataset for Visual Road Assessment**|Natchapon Jongwiriyanurak et.al.|[2408.10872](http://arxiv.org/abs/2408.10872)|null|\n", "2408.10688": "|**2024-08-20**|**TDS-CLIP: Temporal Difference Side Network for Image-to-Video Transfer Learning**|Bin Wang et.al.|[2408.10688](http://arxiv.org/abs/2408.10688)|**[link](https://github.com/BBYL9413/TDS-CLIP)**|\n", "2408.10652": "|**2024-08-20**|**Vocabulary-Free 3D Instance Segmentation with Vision and Language Assistant**|Guofeng Mei et.al.|[2408.10652](http://arxiv.org/abs/2408.10652)|null|\n", "2408.10575": "|**2024-08-20**|**MUSE: Mamba is Efficient Multi-scale Learner for Text-video Retrieval**|Haoran Tang et.al.|[2408.10575](http://arxiv.org/abs/2408.10575)|**[link](https://github.com/hrtang22/MUSE)**|\n", "2408.10433": "|**2024-08-19**|**CLIP-DPO: Vision-Language Models as a Source of Preference for Fixing Hallucinations in LVLMs**|Yassine Ouali et.al.|[2408.10433](http://arxiv.org/abs/2408.10433)|null|\n", "2408.10202": "|**2024-08-19**|**SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP**|Yusuke Hirota et.al.|[2408.10202](http://arxiv.org/abs/2408.10202)|null|\n", "2408.10012": "|**2024-08-19**|**CLIPCleaner: Cleaning Noisy Labels with CLIP**|Chen Feng et.al.|[2408.10012](http://arxiv.org/abs/2408.10012)|null|\n", "2408.09984": "|**2024-08-19**|**Boosting Open-Domain Continual Learning via Leveraging Intra-domain Category-aware Prototype**|Yadong Lu et.al.|[2408.09984](http://arxiv.org/abs/2408.09984)|null|\n", "2408.09916": "|**2024-08-19**|**Attribution Analysis Meets Model Editing: Advancing Knowledge Correction in Vision Language Models with VisEdit**|Qizhou Chen et.al.|[2408.09916](http://arxiv.org/abs/2408.09916)|null|\n", "2408.10845": "|**2024-08-19**|**CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving**|Hidehisa Arai et.al.|[2408.10845](http://arxiv.org/abs/2408.10845)|null|\n", "2408.15098": "|**2024-08-27**|**CLIP-AGIQA: Boosting the Performance of AI-Generated Image Quality Assessment with CLIP**|Zhenchen Tang et.al.1.New Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences|[2408.15098](http://arxiv.org/abs/2408.15098)|null|\n", "2408.11795": "|**2024-08-21**|**EE-MLLM: A Data-Efficient and Compute-Efficient Multimodal Large Language Model**|Feipeng Ma et.al.1. University of Science and Technology of China 2. WeChat, Tencent Inc. 3. Institute of Artificial Intelligence, Hefei Comprehensive National Science Center|[2408.11795](http://arxiv.org/abs/2408.11795)|null|\n", "2408.10578": "|**2024-08-20**|**Where to Fetch: Extracting Visual Scene Representation from Large Pre-Trained Models for Robotic Goal Navigation**|Yu Li et.al.|[2408.10578](http://arxiv.org/abs/2408.10578)|null|\n", "2408.10188": "|**2024-08-21**|**LongVILA: Scaling Long-Context Visual Language Models for Long Videos**|Fuzhao Xue et.al.|[2408.10188](http://arxiv.org/abs/2408.10188)|**[link](https://github.com/nvlabs/vila)**|\n", "2408.15996": "|**2024-08-28**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|Wei-Jhe Huang et.al.1. National Tsing Hua University, Taiwan 2. NVIDIA|[2408.15996](http://arxiv.org/abs/2408.15996)|null|\n", "2408.15823": "|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|Peter Neidlinger et.al.1. Else Kroener Fresenius Center for Digital Health, Technical University Dresden, Dresden, Germany 2. StratifAI GmbH, Dresden, Germany 3. Department for Visceral, Thoracic and Vascular Surgery, University Hospital and Faculty of Medicine Carl Gustav Carus, Technische Universit\u00e4t Dresden, Dresden, Germany 4. National Center for Tumor Diseases Dresden (NCT/UCC), a partnership between DKFZ, Faculty of Medicine and University Hospital Carl Gustav Carus, TUD Dresden University of Technology, and Helmholtz-Zentrum Dresden - Rossendorf (HZDR), Dresden, Germany 5. Division of Clinical Epidemiology and Aging Research, German Cancer Research Center (DKFZ), Heidelberg, Germany|[2408.15823](http://arxiv.org/abs/2408.15823)|null|\n", "2408.15802": "|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|Stefan Denner et.al.1. German Cancer Research Center (DKFZ)|[2408.15802](http://arxiv.org/abs/2408.15802)|null|\n", "2408.15740": "|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**|Tianyi Shang et.al.1. School of Mechanical Engineering, Qilu University of Technology (Shandong Academy of Sciences), Jinan 250353, China 2. Department of Electronic and Information Engineering, Fuzhou University, Fuzhou 350100, China 3. School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai 200030, China 4. School of Mechanical Engineering, Tongji University, Shanghai 201804, China|[2408.15740](http://arxiv.org/abs/2408.15740)|null|\n", "2408.15626": "|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|Bianca Lamm et.al.1. MarkantServicesInternationalGmbH, Offenburg, Germany 2. Offenburg University, Germany 3. Institute for Machine Learning and Analytics (IMLA)|[2408.15626](http://arxiv.org/abs/2408.15626)|null|\n", "2408.15566": "|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|Jinglun Li et.al.1. Shanghai Engineering Research Center of AI & Robotics, Academy for Information Processing, School of Engineering & Technology, Fudan University, Shanghai, China 2. School of Computer Science, Fudan University, Shanghai, China 3. Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China 4. Engineering Research Center of AI & Robotics, Ministry of Education, Academy for Engineering & Technology, Fudan University, Shanghai, China 5. Shanghai Key Lab of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China|[2408.15566](http://arxiv.org/abs/2408.15566)|null|\n", "2408.15542": "|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|Jiajun Liu et.al.1.Meituan Group, Beijing 100102, China 2.School of Electronic, Electrical and Communication Engineering, the University of Chinese Academy of Sciences, (UCAS), Beijing, 100049, China 3.Key Laboratory of System Software (Chinese Academy of Sciences) and State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, 100190, China 4.University of Chinese Academy of Sciences, Beijing, 100049, China 5.Meituan Group|[2408.15542](http://arxiv.org/abs/2408.15542)|null|\n", "2408.15521": "|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|Seonghoon Yu et.al.1.AI Graduate School, GIST, South Korea 2.NAVER Cloud, South Korea 3.Electrical Engineering and Computer Science, GIST, South Korea|[2408.15521](http://arxiv.org/abs/2408.15521)|null|\n", "2408.15518": "|**2024-08-28**|**Dolphin: Long Context as a New Modality for Energy-Efficient On-Device Language Models**|Wei Chen et.al.1.NexaAI, Sunnyvale, CA 94086 2.NexaAI, Sunnyvale, CA 94086 3.NexaAI, Sunnyvale, CA 94086 4.NexaAI, Sunnyvale, CA 94086|[2408.15518](http://arxiv.org/abs/2408.15518)|null|\n", "2408.15511": "|**2024-08-28**|**AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training, Finetuning, and Evaluating Aerospace Embodied World Models**|Fanglong Yao et.al.1. Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China 2. Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing 100190, China 3. University of Chinese Academy of Sciences, Beijing 100190, China 4. School of Electronic, Electrical and Communication Engineering, University of Chinese Academy of Sciences, Beijing 100190, China 5. National Natural Science Foundation of China|[2408.15511](http://arxiv.org/abs/2408.15511)|null|\n", "2408.15305": "|**2024-08-27**|**Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis**|Sakhinana Sagar Srinivas et.al.1.TCS Research, Bangalore 2.IIT Dharwad 3.IIIT Pune|[2408.15305](http://arxiv.org/abs/2408.15305)|null|\n"}}
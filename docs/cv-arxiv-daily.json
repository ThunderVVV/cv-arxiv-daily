{"MLLM": {"2408.16769": "|**2024-08-29**|**PromptSmooth: Certifying Robustness of Medical Vision-Language Models via Prompt Learning**|1. Mohamed Bin Zayed <br>University of Artificial <br>Intelligence|[2408.16769](http://arxiv.org/abs/2408.16769)|**[link](https://github.com/nhussein/promptsmooth)**|\n", "2408.16730": "|**2024-08-29**|**VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths Vision Computation**|1. University of Science and <br>Technology of China 2. Show <br>Lab, National University of <br>Singapore 3. Xiaohongshu.Inc <br>4. Institute for Infocomm <br>Research, A*STAR|[2408.16730](http://arxiv.org/abs/2408.16730)|null|\n", "2408.16662": "|**2024-08-29**|**Space3D-Bench: Spatial 3D Question Answering Benchmark**|1. ETH Z\u00fcrich 2. Microsoft|[2408.16662](http://arxiv.org/abs/2408.16662)|null|\n", "2408.16647": "|**2024-08-29**|**DriveGenVLM: Real-world Video Generation for Vision Language Model based Autonomous Driving**|1. Columbia University 2. <br>Columbia University 3. <br>Columbia University 4. <br>Columbia University|[2408.16647](http://arxiv.org/abs/2408.16647)|null|\n", "2408.16500": "|**2024-08-29**|**CogVLM2: Visual Language Models for Image and Video Understanding**|1. ZhipuAI 2. Tsinghua <br>University|[2408.16500](http://arxiv.org/abs/2408.16500)|**[link](https://github.com/thudm/cogvlm2)**|\n", "2408.16486": "|**2024-08-29**|**Adapting Vision-Language Models to Open Classes via Test-Time Prompt Tuning**|1. Institute of Automation, <br>Chinese Academy of Sciences 2. <br>School of Artificial <br>Intelligence, University of <br>Chinese Academy of Sciences|[2408.16486](http://arxiv.org/abs/2408.16486)|null|\n", "2408.16412": "|**2024-08-29**|**Text-Enhanced Zero-Shot Action Recognition: A training-free approach**|1. University of Trento 2. <br>Fondazione Bruno Kessler (FBK) <br>3. University of Bologna|[2408.16412](http://arxiv.org/abs/2408.16412)|null|\n", "2408.16296": "|**2024-08-29**|**Rethinking Sparse Lexical Representations for Image Retrieval in the Age of Rising Multi-Modal Large Language Models**|1. Kioxia Corporation|[2408.16296](http://arxiv.org/abs/2408.16296)|null|\n", "2408.16228": "|**2024-08-29**|**Policy Adaptation via Language Optimization: Decomposing Tasks for Few-Shot Imitation**|1. University of California, <br>Berkeley|[2408.16228](http://arxiv.org/abs/2408.16228)|null|\n", "2408.16224": "|**2024-08-30**|**LLaVA-SG: Leveraging Scene Graphs as Visual Semantic Expression in Vision-Language Models**|1.Tsinghua University 2.Xiaomi <br>AI Lab|[2408.16224](http://arxiv.org/abs/2408.16224)|null|\n", "2408.16219": "|**2024-08-29**|**Training-free Video Temporal Grounding using Large-scale Pre-trained Models**|1. Wangxuan Institute of <br>Computer Technology, Peking <br>University 2. National <br>Institute of Health Data <br>Science, Peking University 3. <br>State Key Laboratory of <br>General Artificial <br>Intelligence, Peking <br>University|[2408.16219](http://arxiv.org/abs/2408.16219)|null|\n", "2408.16176": "|**2024-08-28**|**VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images**|1.VirginiaTech <br>2.Univ.ofCalifornia,Irvine <br>3.UNCatChapelHill <br>4.TulaneUniv.|[2408.16176](http://arxiv.org/abs/2408.16176)|**[link](https://github.com/sammarfy/vlm4bio)**|\n", "2408.15996": "|**2024-08-29**|**Spatio-Temporal Context Prompting for Zero-Shot Action Detection**|1. National Tsing Hua <br>University 2. NVIDIA|[2408.15996](http://arxiv.org/abs/2408.15996)|null|\n", "2408.15823": "|**2024-08-28**|**Benchmarking foundation models as feature extractors for weakly-supervised computational pathology**|1. Technical University <br>Dresden 2. StratifAI GmbH 3. <br>German Cancer Research Center <br>4. Johannes Kepler University|[2408.15823](http://arxiv.org/abs/2408.15823)|null|\n", "2408.15802": "|**2024-08-28**|**Visual Prompt Engineering for Medical Vision Language Models in Radiology**|1. German Cancer Research <br>Center (DKFZ)|[2408.15802](http://arxiv.org/abs/2408.15802)|null|\n", "2408.15740": "|**2024-08-28**|**MambaPlace:Text-to-Point-Cloud Cross-Modal Place Recognition with Attention Mamba Mechanisms**| |[2408.15740](http://arxiv.org/abs/2408.15740)|**[link](https://github.com/CV4RA/MambaPlace)**|\n", "2408.15626": "|**2024-08-28**|**Can Visual Language Models Replace OCR-Based Visual Question Answering Pipelines in Production? A Case Study in Retail**|1. <br>MarkantServicesInternationalGm <br>bH 2. OffenburgUniversity 3. <br>InstituteforMachineLearningand <br>Analytics(IMLA)|[2408.15626](http://arxiv.org/abs/2408.15626)|null|\n", "2408.15566": "|**2024-08-28**|**TagOOD: A Novel Approach to Out-of-Distribution Detection via Vision-Language Representations and Class Center Learning**|1. Fudan University 2. <br>Shanghai Engineering Research <br>Center of AI & Robotics 3. <br>Shanghai Key Lab of <br>Intelligent Information <br>Processing 4. Academy for <br>Engineering and Technology|[2408.15566](http://arxiv.org/abs/2408.15566)|**[link](https://github.com/Jarvisgivemeasuit/tagood)**|\n", "2408.15542": "|**2024-08-28**|**Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input**|1.Meituan Group 2.University <br>of Chinese Academy of Sciences <br>3.Institute of Software, <br>Chinese Academy of Sciences <br>4.Key Laboratory of System <br>Software|[2408.15542](http://arxiv.org/abs/2408.15542)|null|\n", "2408.15521": "|**2024-08-28**|**A Simple Baseline with Single-encoder for Referring Image Segmentation**|1.GIST 2.NAVER Cloud <br>3.Electrical Engineering and <br>Computer Science|[2408.15521](http://arxiv.org/abs/2408.15521)|null|\n", "2409.03757": "|**2024-09-05**|**Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding**|1. University of Illinois <br>Urbana-Champaign 2. Carnegie <br>Mellon University|[2409.03757](http://arxiv.org/abs/2409.03757)|**[link](https://github.com/yunzeman/lexicon3d)**|\n", "2409.03583": "|**2024-09-05**|**Text-Guided Mixup Towards Long-Tailed Image Categorization**|1. University of Washington 2. <br>Alibaba Group|[2409.03583](http://arxiv.org/abs/2409.03583)|**[link](https://github.com/rsamf/text-guided-mixup)**|\n", "2409.03525": "|**2024-09-05**|**FrozenSeg: Harmonizing Frozen Foundation Models for Open-Vocabulary Segmentation**|1. Harbin Institute of <br>Technology 2. University of <br>Surrey 3. Nanyang <br>Technological University|[2409.03525](http://arxiv.org/abs/2409.03525)|null|\n", "2409.03521": "|**2024-09-05**|**Have Large Vision-Language Models Mastered Art History?**|1.KULeuven 2.TUDelft|[2409.03521](http://arxiv.org/abs/2409.03521)|null|\n", "2409.03272": "|**2024-09-05**|**OccLLaMA: An Occupancy-Language-Action Generative World Model for Autonomous Driving**|1.FudanUniversity <br>2.TsinghuaUniversity|[2409.03272](http://arxiv.org/abs/2409.03272)|null|\n", "2409.02914": "|**2024-09-04**|**Can LVLMs Obtain a Driver's License? A Benchmark Towards Reliable AGI for Autonomous Driving**|1. ShanghaiTech University 2. <br>The Chinese University of Hong <br>Kong|[2409.02914](http://arxiv.org/abs/2409.02914)|null|\n", "2409.02882": "|**2024-09-04**|**Benchmarking Spurious Bias in Few-Shot Image Classifiers**|1. University of Virginia|[2409.02882](http://arxiv.org/abs/2409.02882)|**[link](https://github.com/gtzheng/fewstab)**|\n", "2409.02664": "|**2024-09-04**|**Standing on the Shoulders of Giants: Reprogramming Visual-Language Model for General Deepfake Detection**|1. Shenzhen University 2. <br>Tencent Youtu Lab|[2409.02664](http://arxiv.org/abs/2409.02664)|null|\n", "2409.02561": "|**2024-09-04**|**Vision-Language Navigation with Continual Learning**|1. State Key Laboratory of <br>Multimodal Artificial <br>Intelligence Systems 2. <br>Institute of Automation, <br>Chinese Academy of Sciences <br>(CASIA) 3. University of <br>Chinese Academy of Sciences <br>(UCAS)|[2409.02561](http://arxiv.org/abs/2409.02561)|null|\n", "2409.02522": "|**2024-09-04**|**Cog-GA: A Large Language Models-based Generative Agent for Vision-Language Navigation in Continuous Environments**|1. State Key Laboratory of <br>Multimodal Artificial <br>Intelligence Systems, <br>Institute of Automation, <br>Chinese Academy of Science <br>(CASIA), Beijing 2. University <br>of Chinese Academy of Sciences <br>(UCAS), Beijing 3. Department <br>of Computer Science, The <br>University of Hong Kong, Hong <br>Kong Special Administrative <br>Region (SAR)|[2409.02522](http://arxiv.org/abs/2409.02522)|null|\n", "2409.02418": "|**2024-09-04**|**MOSMOS: Multi-organ segmentation facilitated by medical report supervision**|1.Fudan University 2.The Hong <br>Kong University of Science and <br>Technology 3.Second Military <br>Medical University (Naval <br>Medical University) 4.Shanghai <br>Collaborative Innovation <br>Center of Intelligent Visual <br>Computing|[2409.02418](http://arxiv.org/abs/2409.02418)|null|\n", "2409.02389": "|**2024-09-04**|**Multi-modal Situated Reasoning in 3D Scenes**|1. State Key Laboratory of <br>General Artificial <br>Intelligence, BIGAI 2. Peking <br>University|[2409.02389](http://arxiv.org/abs/2409.02389)|null|\n", "2409.02278": "|**2024-09-03**|**Evaluation and Comparison of Visual Language Models for Transportation Engineering Problems**|1. Indian Institute of <br>Technology Kanpur 2. New York <br>University 3. Indian Institute <br>of Technology Kanpur 4. Indian <br>Institute of Technology Kanpur|[2409.02278](http://arxiv.org/abs/2409.02278)|null|\n", "2409.02253": "|**2024-09-03**|**How to Determine the Preferred Image Distribution of a Black-Box Vision-Language Model?**|1.AutodeskAIResearch|[2409.02253](http://arxiv.org/abs/2409.02253)|null|\n", "2409.02101": "|**2024-09-03**|**Towards Real-World Adverse Weather Image Restoration: Enhancing Clearness and Semantics with Vision-Language Models**|1.The Chinese University of <br>Hong Kong 2.Shanghai <br>Artificial Intelligence <br>Laboratory|[2409.02101](http://arxiv.org/abs/2409.02101)|**[link](https://github.com/jiaqixuac/WResVLM)**|\n", "2409.02084": "|**2024-09-03**|**GraspSplats: Efficient Manipulation with 3D Feature Splatting**|1. UCSanDiego|[2409.02084](http://arxiv.org/abs/2409.02084)|null|\n", "2409.01883": "|**2024-09-03**|**Boosting Vision-Language Models for Histopathology Classification: Predict all at once**|1. Universit\u00e9 Catholique de <br>Louvain (UCLouvain) 2. <br>Universit\u00e9 de Mons (UMons) 3. <br>\u00c9cole de Technologie <br>Sup\u00e9rieure (\u00c9TS) 4. Centre de <br>Recherche du Centre <br>Hospitalier de l\u2019Universit\u00e9 de <br>Montr\u00e9al (CRCHUM)|[2409.01883](http://arxiv.org/abs/2409.01883)|**[link](https://github.com/fereshteshakeri/histo-transclip)**|\n", "2409.02958": "|**2024-09-03**|**Multi-Modal Adapter for Vision-Language Models**|1. University of Amsterdam 2. <br>Vinted|[2409.02958](http://arxiv.org/abs/2409.02958)|**[link](https://github.com/dqmis/clip-mma)**|\n", "2409.01835": "|**2024-09-03**|**Towards Generative Class Prompt Learning for Few-shot Visual Recognition**|1. University of North <br>Carolina at Chapel Hill 2. <br>Computer Vision Center & <br>Computer Science Department, <br>Universitat Aut\u00f2noma de <br>Barcelona 3. MICC, University <br>of Florence 4. University of <br>Florence|[2409.01835](http://arxiv.org/abs/2409.01835)|null|\n", "2409.01821": "|**2024-09-04**|**When Does Visual Prompting Outperform Linear Probing for Vision-Language Models? A Likelihood Perspective**|1. National Tsing Hua <br>University 2. Dartmouth <br>College 3. IBM Research 4. The <br>Chinese University of Hong <br>Kong|[2409.01821](http://arxiv.org/abs/2409.01821)|**[link](https://github.com/ibm/vp-llr)**|\n", "2409.12191": "|**2024-09-18**|**Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution**|1.QwenTeamAlibabaGroup|[2409.12191](http://arxiv.org/abs/2409.12191)|**[link](https://github.com/qwenlm/qwen2-vl)**|\n", "2409.12011": "|**2024-09-18**|**Mixture of Prompt Learning for Vision Language Models**|1.Tsinghua University|[2409.12011](http://arxiv.org/abs/2409.12011)|null|\n", "2409.11941": "|**2024-09-18**|**GauTOAO: Gaussian-based Task-Oriented Affordance of Objects**|1.Peking University|[2409.11941](http://arxiv.org/abs/2409.11941)|null|\n", "2409.11919": "|**2024-09-18**|**LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Foundation Models**|1.Valeo.ai 2.Univ. Grenoble <br>Alpes 3.Sorbonne Universit\u00e9|[2409.11919](http://arxiv.org/abs/2409.11919)|null|\n", "2409.11656": "|**2024-09-18**|**VL-Reader: Vision and Language Reconstructor is an Effective Scene Text Recognizer**|1.AlibabaGroup <br>2.HuazhongUniversityofSciencea <br>ndTechnology 3.AlibabaGroup <br>4.HuazhongUniversityofSciencea <br>ndTechnology|[2409.11656](http://arxiv.org/abs/2409.11656)|null|\n", "2409.11513": "|**2024-09-17**|**Mamba Fusion: Learning Actions Through Questioning**|1. <br>GeorgiaInstituteofTechnology <br>2. StonyBrookUniversity 3. <br>GoogleResearch|[2409.11513](http://arxiv.org/abs/2409.11513)|**[link](https://github.com/dongzhikang/mambavl)**|\n", "2409.11402": "|**2024-09-17**|**NVLM: Open Frontier-Class Multimodal LLMs**|1.NVIDIA|[2409.11402](http://arxiv.org/abs/2409.11402)|null|\n", "2409.11363": "|**2024-09-17**|**CORE-Bench: Fostering the Credibility of Published Research Through a Computational Reproducibility Agent Benchmark**|1.Princeton University|[2409.11363](http://arxiv.org/abs/2409.11363)|**[link](https://github.com/siegelz/core-bench)**|\n", "2409.11323": "|**2024-09-17**|**LPT++: Efficient Training on Mixture of Long-tailed Experts**|1. School of Computer Science <br>and Technology, Harbin <br>Institute of Technology 2. <br>School of Computing and <br>Information Systems, Singapore <br>Management University 3. The <br>Hong Kong Polytechnic <br>University|[2409.11323](http://arxiv.org/abs/2409.11323)|null|\n", "2409.11148": "|**2024-09-17**|**Improving the Efficiency of Visually Augmented Language Models**|1.UniversityoftheBasqueCountry <br>UPV/EHU|[2409.11148](http://arxiv.org/abs/2409.11148)|null|\n", "2409.11007": "|**2024-09-17**|**CAST: Cross-modal Alignment Similarity Test for Vision Language Models**|1. University of Edinburgh 2. <br>University of Trento|[2409.11007](http://arxiv.org/abs/2409.11007)|**[link](https://github.com/gautierdag/cast)**|\n", "2409.10921": "|**2024-09-17**|**KALE: An Artwork Image Captioning System Augmented with Heterogeneous Graph**|1.TheUniversityofMelbourne|[2409.10921](http://arxiv.org/abs/2409.10921)|**[link](https://github.com/yanbei-jiang/artwork-interpretation)**|\n", "2409.10719": "|**2024-09-16**|**Benchmarking VLMs' Reasoning About Persuasive Atypical Images**| |[2409.10719](http://arxiv.org/abs/2409.10719)|null|\n", "2409.10683": "|**2024-09-16**|**MotIF: Motion Instruction Fine-tuning**|1. Massachusetts Institute of <br>Technology 2. Stanford <br>University 3. Carnegie Mellon <br>University|[2409.10683](http://arxiv.org/abs/2409.10683)|null|\n", "2409.10488": "|**2024-09-16**|**Do Pre-trained Vision-Language Models Encode Object States?**|1.BrownUniversity|[2409.10488](http://arxiv.org/abs/2409.10488)|null|\n", "2409.10441": "|**2024-09-16**|**CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera**|1. University of <br>CaliforniaSanDiego 2. <br>IntelLabs|[2409.10441](http://arxiv.org/abs/2409.10441)|null|\n", "2409.10419": "|**2024-09-16**|**HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models**|1.NewYorkUniversity|[2409.10419](http://arxiv.org/abs/2409.10419)|null|\n", "2409.10283": "|**2024-09-16**|**ASMA: An Adaptive Safety Margin Algorithm for Vision-Language Drone Navigation via Scene-Aware Control Barrier Functions**|1.Purdue University|[2409.10283](http://arxiv.org/abs/2409.10283)|null|\n", "2409.10196": "|**2024-09-16**|**NEUSIS: A Compositional Neuro-Symbolic Framework for Autonomous Perception, Reasoning, and Planning in Complex UAV Search Missions**|1. Monash University 2. Monash <br>University 3. Monash <br>University 4. Monash <br>University|[2409.10196](http://arxiv.org/abs/2409.10196)|null|\n", "2409.10090": "|**2024-09-16**|**MotionCom: Automatic and Motion-Aware Image Composition with LLM and Video Diffusion Prior**|1.NanyangTechnologicalUniversi <br>ty 2.AlibabaGroup|[2409.10090](http://arxiv.org/abs/2409.10090)|**[link](https://github.com/weijing-tao/MotionCom)**|\n", "2409.17143": "|**2024-09-25**|**Attention Prompting on Image for Large Vision-Language Models**|1. National University of <br>Singapore|[2409.17143](http://arxiv.org/abs/2409.17143)|**[link](https://github.com/yu-rp/apiprompting)**|\n", "2409.17126": "|**2024-09-25**|**Blox-Net: Generative Design-for-Robot-Assembly Using VLM Supervision, Physics Simulation, and a Robot with Reset**|1.UCBerkeley <br>2.CornellUniversity|[2409.17126](http://arxiv.org/abs/2409.17126)|null|\n", "2409.17080": "|**2024-09-25**|**Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?**|1.Groundlight|[2409.17080](http://arxiv.org/abs/2409.17080)|null|\n", "2409.17045": "|**2024-09-25**|**GeoBiked: A Dataset with Geometric Features and Automated Labeling Techniques to Enable Deep Generative Models in Engineering Design**|1.UniversityofAugsburg <br>2.BMWGroup|[2409.17045](http://arxiv.org/abs/2409.17045)|null|\n", "2409.16718": "|**2024-09-25**|**Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification**|1.TheUniversityofTokyo <br>2.UniversityofSouthernCaliforn <br>ia <br>3.RIKENCenterforAdvancedIntell <br>igenceProject <br>4.TheChineseUniversityofHongKo <br>ng|[2409.16718](http://arxiv.org/abs/2409.16718)|**[link](https://github.com/minglllli/clipfit)**|\n", "2409.16604": "|**2024-09-25**|**Semi-LLIE: Semi-supervised Contrastive Learning with Mamba-based Low-light Image Enhancement**|1. Northwestern Polytechnical <br>University 2. Science and <br>Technology on <br>Electromechanical Dynamic <br>Control Laboratory 3. School <br>of Astronautics 4. Key <br>Laboratory of Intelligent <br>Interaction and Applications|[2409.16604](http://arxiv.org/abs/2409.16604)|null|\n", "2409.16494": "|**2024-09-24**|**A Unified Hallucination Mitigation Framework for Large Vision-Language Models**|1.The University of Texas at <br>Dallas|[2409.16494](http://arxiv.org/abs/2409.16494)|null|\n", "2409.16484": "|**2024-09-24**|**BehAV: Behavioral Rule Guided Autonomy Using VLMs for Robot Navigation in Outdoor Scenes**|1. University of Maryland|[2409.16484](http://arxiv.org/abs/2409.16484)|null|\n", "2409.16278": "|**2024-09-24**|**Semantic Refocused Tuning for Open-Vocabulary Panoptic Segmentation**|1.TsinghuaUniversity <br>2.BoschCorporateResearch|[2409.16278](http://arxiv.org/abs/2409.16278)|null|\n", "2409.16183": "|**2024-09-24**|**Expert-level vision-language foundation model for real-world radiology and comprehensive evaluation**|1.Sun Yat-Sen University <br>2.Beijing University of Posts <br>and Telecommunications <br>3.Peking University|[2409.16183](http://arxiv.org/abs/2409.16183)|null|\n", "2409.16159": "|**2024-09-24**|**ComiCap: A VLMs pipeline for dense captioning of Comic Panels**|1. Computer Vision Center, UAB <br>2. University of Florence 3. <br>MICC 4. University of Florence|[2409.16159](http://arxiv.org/abs/2409.16159)|**[link](https://github.com/emanuelevivoli/comicap)**|\n", "2409.16024": "|**2024-09-24**|**Bridging Environments and Language with Rendering Functions and Vision-Language Models**|1. NAVER LABS Europe 2. <br>Institute of Intelligent <br>Systems and Robotics, Sorbonne <br>University|[2409.16024](http://arxiv.org/abs/2409.16024)|null|\n", "2409.15922": "|**2024-09-24**|**Overcoming Reward Model Noise in Instruction-Guided Reinforcement Learning**|1.TheUniversityofMelbourne|[2409.15922](http://arxiv.org/abs/2409.15922)|null|\n", "2409.15658": "|**2024-09-24**|**ReLEP: A Novel Framework for Real-world Long-horizon Embodied Planning**|1.PekingUniversity 2.|[2409.15658](http://arxiv.org/abs/2409.15658)|null|\n", "2409.15574": "|**2024-09-23**|**Clinical-grade Multi-Organ Pathology Report Generation for Multi-scale Whole Slide Images via a Semantically Guided Medical Text Foundation Model**|1. Korea University 2. The <br>Catholic University of Korea <br>3. Korea University Anam <br>Hospital|[2409.15574](http://arxiv.org/abs/2409.15574)|**[link](https://github.com/hvcl/Clinical-grade-Pathology-Report-Generation)**|\n", "2409.15505": "|**2024-09-23**|**Discovering Object Attributes by Prompting Large Language Models with Perception-Action APIs**|1. University of Maryland|[2409.15505](http://arxiv.org/abs/2409.15505)|null|\n", "2409.15486": "|**2024-09-23**|**VLMine: Long-Tail Data Mining with Vision Language Models**|1.CruiseLLC 2.MetaInc.|[2409.15486](http://arxiv.org/abs/2409.15486)|null|\n", "2409.15256": "|**2024-09-23**|**Behavioral Bias of Vision-Language Models: A Behavioral Finance View**|1. University of Southern <br>California|[2409.15256](http://arxiv.org/abs/2409.15256)|**[link](https://github.com/mydcxiao/vlm_behavioral_fin)**|\n", "2409.15250": "|**2024-09-23**|**ReVLA: Reverting Visual Domain Limitation of Robotic Foundation Models**|1.INSAIT, Sofia University|[2409.15250](http://arxiv.org/abs/2409.15250)|null|\n", "2409.16084": "|**2024-09-24**|**MM-CamObj: A Comprehensive Multimodal Dataset for Camouflaged Object Scenarios**|1.ShanghaiJiaoTongUniversity <br>2.InstituteforAdvancedAlgorith <br>msResearch|[2409.16084](http://arxiv.org/abs/2409.16084)|**[link](https://github.com/jcruan519/mm-camobj)**|\n", "2409.18125": "|**2024-09-26**|**LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness**|1.TheUniversityofHongKong <br>2.ShanghaiAILaboratory|[2409.18125](http://arxiv.org/abs/2409.18125)|null|\n", "2409.18082": "|**2024-09-26**|**SKT: Integrating State-Aware Keypoint Trajectories with Vision-Language Models for Robotic Garment Manipulation**|1.Shanghai Jiao Tong <br>University 2.The Hong Kong <br>University of Science and <br>Technology 3.Department of <br>Computer Science, National <br>University of Singapore <br>4.CUHK-MMLab|[2409.18082](http://arxiv.org/abs/2409.18082)|null|\n", "2409.18060": "|**2024-09-26**|**Infering Alt-text For UI Icons With Large Language Models During App Development**|1. University of Texas at <br>Arlington 2. University of <br>Texas at Arlington|[2409.18060](http://arxiv.org/abs/2409.18060)|null|\n", "2409.18042": "|**2024-09-26**|**EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions**|1.HongKongUniversityofSciencea <br>ndTechnology <br>2.TheUniversityofHongKong <br>3.HuaweiNoah\u2019sArkLab <br>4.TheChineseUniversityofHongKo <br>ng|[2409.18042](http://arxiv.org/abs/2409.18042)|null|\n", "2409.18023": "|**2024-09-26**|**DARE: Diverse Visual Question Answering with Robustness Evaluation**|1. University of Cambridge 2. <br>Google DeepMind|[2409.18023](http://arxiv.org/abs/2409.18023)|null|\n", "2409.17958": "|**2024-09-26**|**The Hard Positive Truth about Vision-Language Compositionality**|1. UniversityofWashington 2. <br>UniversityofCalifornia,LosAnge <br>les 3. AllenInstituteforAI|[2409.17958](http://arxiv.org/abs/2409.17958)|null|\n", "2409.17805": "|**2024-09-26**|**Cascade Prompt Learning for Vision-Language Model Adaptation**|1.Nankai University 2.Shenzhen <br>Futian 3.Megvii Technology|[2409.17805](http://arxiv.org/abs/2409.17805)|null|\n", "2409.17727": "|**2024-09-26**|**Robotic-CLIP: Fine-tuning CLIP on Action Data for Robotic Applications**|1.FPTSoftwareAICenter <br>2.Automation&ControlInstitute( <br>ACIN),TUWien <br>3.CenterforVision,Automation&C <br>ontrol,AITAustrianInstituteofT <br>echnology(GmbH) <br>4.TheUniversityofTokyo|[2409.17727](http://arxiv.org/abs/2409.17727)|null|\n", "2409.17663": "|**2024-09-26**|**Explanation Bottleneck Models**|1. NTT 2. Kyoto University|[2409.17663](http://arxiv.org/abs/2409.17663)|**[link](https://github.com/yshinya6/xbm)**|\n", "2409.17641": "|**2024-09-26**|**AP-VLM: Active Perception Enabled by Vision-Language Models**|1. University of Surrey|[2409.17641](http://arxiv.org/abs/2409.17641)|null|\n", "2409.17634": "|**2024-09-26**|**P4Q: Learning to Prompt for Quantization in Visual-language Models**|1.BeihangUniversity <br>2.XiaohongshuInc <br>3.ZhongguancunLaboratory|[2409.17634](http://arxiv.org/abs/2409.17634)|null|\n", "2409.17621": "|**2024-09-26**|**Leveraging Semantic and Geometric Information for Zero-Shot Robot-to-Human Handover**|1.SouthernUniversityofSciencea <br>ndTechnology <br>2.JiaxingResearchInstitute <br>3.ShenzhenKeyLaboratoryofRobot <br>icsPerceptionandIntelligence <br>4.ElectronicandElectricalEngin <br>eering|[2409.17621](http://arxiv.org/abs/2409.17621)|null|\n", "2409.17610": "|**2024-09-26**|**ZALM3: Zero-Shot Enhancement of Vision-Language Alignment via In-Context Information in Multi-Turn Multimodal Medical Dialogue**|1.Tsinghua Shenzhen <br>International Graduate School <br>2.Tianjin University <br>3.PAIIInc. 4.PingAn Technology|[2409.17610](http://arxiv.org/abs/2409.17610)|null|\n", "2409.17519": "|**2024-09-26**|**Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization**|1.TheUniversityofTokyo|[2409.17519](http://arxiv.org/abs/2409.17519)|null|\n", "2409.17486": "|**2024-09-26**|**Global-Local Medical SAM Adaptor Based on Full Adaption**|1.LiaoningTechnicalUniversity <br>2.NortheasternUniversity <br>3.ChinaMedicalUniversity|[2409.17486](http://arxiv.org/abs/2409.17486)|null|\n", "2409.17457": "|**2024-09-26**|**CadVLM: Bridging Language and Vision in the Generation of Parametric CAD Sketches**|1. Universit\u00e9 de Montr\u00e9al & <br>MILA 2. Autodesk AI Lab|[2409.17457](http://arxiv.org/abs/2409.17457)|null|\n", "2409.17330": "|**2024-09-25**|**VL4AD: Vision-Language Models Improve Pixel-wise Anomaly Detection**|1.CARIAD SE 2.Technical <br>University Berlin|[2409.17330](http://arxiv.org/abs/2409.17330)|null|\n", "2409.17313": "|**2024-09-25**|**Navigating the Nuances: A Fine-grained Evaluation of Vision-Language Navigation**|1.KULeuven 2.PekingUniversity <br>3.NanyangTechnologicalUniversi <br>ty 4.FudanUniversity|[2409.17313](http://arxiv.org/abs/2409.17313)|null|\n", "2410.01744": "|**2024-10-03**|**Leopard: A Vision Language Model For Text-Rich Multi-Image Tasks**|1. University of Notre Dame 2. <br>Tencent AI Seattle Lab 3. UIUC|[2410.01744](http://arxiv.org/abs/2410.01744)|null|\n", "2410.01690": "|**2024-10-02**|**Why context matters in VQA and Reasoning: Semantic interventions for VLM input modalities**|1.ETHZurich 2.ETHAICenter <br>3.GermanCancerResearchCenter(D <br>KFZ) 4.IBMResearch|[2410.01690](http://arxiv.org/abs/2410.01690)|null|\n", "2410.01620": "|**2024-10-03**|**LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large Vision-Language Models**|1. Yale University 2. Imperial <br>College London 3. Australian <br>National University 4. <br>University of Georgia|[2410.01620](http://arxiv.org/abs/2410.01620)|null|\n", "2410.01534": "|**2024-10-02**|**Toward a Holistic Evaluation of Robustness in CLIP Models**|1.TheAustralianNationalUnivers <br>ity 2.TheUniversityofO\u00b4buda <br>3.CurtinUniversity|[2410.01534](http://arxiv.org/abs/2410.01534)|null|\n", "2410.01506": "|**2024-10-02**|**LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion**|1.AustralianNationalUniversity <br>2.Data61/CSIRO <br>3.CurtinUniversity|[2410.01506](http://arxiv.org/abs/2410.01506)|null|\n", "2410.01438": "|**2024-10-02**|**Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness on Vision Language Models**|1.AcademiaSinica <br>2.NationalTaiwanUniversity <br>3.NationalYangMingChiaoTungUni <br>versity|[2410.01438](http://arxiv.org/abs/2410.01438)|null|\n", "2410.01407": "|**2024-10-02**|**AgriCLIP: Adapting CLIP for Agriculture and Livestock via Domain-Specialized Cross-Model Alignment**|1.MBZUAI|[2410.01407](http://arxiv.org/abs/2410.01407)|null|\n", "2410.01345": "|**2024-10-02**|**Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark and LLM-guided 3D Policy**|1.Inria 2.\u00c9cole normale <br>sup\u00e9rieure 3.CNRS 4.PSL <br>Research University|[2410.01345](http://arxiv.org/abs/2410.01345)|null|\n", "2410.01264": "|**2024-10-02**|**Backdooring Vision-Language Models with Out-Of-Distribution Data**|1.StonyBrookUniversity <br>2.KingAbdullahUniversityofScie <br>nceandTechnology|[2410.01264](http://arxiv.org/abs/2410.01264)|null|\n", "2410.01261": "|**2024-10-02**|**OCC-MLLM:Empowering Multimodal Large Language Model For the Understanding of Occluded Objects**|1. University of Toronto|[2410.01261](http://arxiv.org/abs/2410.01261)|null|\n", "2410.01180": "|**2024-10-02**|**UAL-Bench: The First Comprehensive Unusual Activity Localization Benchmark**|1.Texas A&M University <br>2.University of Macau|[2410.01180](http://arxiv.org/abs/2410.01180)|null|\n", "2410.01023": "|**2024-10-01**|**Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!**|1. Yonsei University|[2410.01023](http://arxiv.org/abs/2410.01023)|null|\n", "2410.00982": "|**2024-10-01**|**ScVLM: a Vision-Language Model for Driving Safety Critical Event Understanding**|1. Virginia Polytechnic <br>Institute and State University <br>2. Virginia Tech <br>Transportation Institute 3. <br>Department of Statistics|[2410.00982](http://arxiv.org/abs/2410.00982)|null|\n", "2410.00905": "|**2024-10-01**|**Removing Distributional Discrepancies in Captions Improves Image-Text Alignment**|1. University of <br>Wisconsin-Madison 2. Adobe <br>Research|[2410.00905](http://arxiv.org/abs/2410.00905)|null|\n", "2410.00731": "|**2024-10-01**|**Improved Generation of Synthetic Imaging Data Using Feature-Aligned Diffusion**|1.Boston|[2410.00731](http://arxiv.org/abs/2410.00731)|null|\n", "2410.00448": "|**2024-10-01**|**Advancing Medical Radiograph Representation Learning: A Hybrid Pre-training Paradigm with Multilevel Semantic Granularity**|1.The Chinese University of <br>Hong Kong, Shenzhen 2.The <br>University of Georgia <br>3.Northwestern Polytechnical <br>University 4.University of <br>Illinois at Urbana-Champaign|[2410.00448](http://arxiv.org/abs/2410.00448)|null|\n", "2410.00388": "|**2024-10-01**|**Find Everything: A General Vision Language Model Approach to Multi-Object Search**|1. University of Toronto 2. <br>Syncere AI|[2410.00388](http://arxiv.org/abs/2410.00388)|null|\n", "2410.00371": "|**2024-10-01**|**AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation**|1.NVIDIA <br>2.UniversityofWashington <br>3.UniversidadCat\u00f3licaSanPablo <br>4.MIT|[2410.00371](http://arxiv.org/abs/2410.00371)|null|\n", "2410.00332": "|**2024-10-01**|**Vision Language Models Know Law of Conservation without Understanding More-or-Less**|1. University of Michigan 2. <br>University of North Carolina <br>at Chapel Hill 3. Johns <br>Hopkins University 4. <br>University of California, San <br>Diego|[2410.00332](http://arxiv.org/abs/2410.00332)|null|\n", "2410.00324": "|**2024-10-01**|**Vision Language Models See What You Want but not What You See**|1. Johns Hopkins University 2. <br>University of California, San <br>Diego 3. University of North <br>Carolina at Chapel Hill 4. <br>University of Michigan|[2410.00324](http://arxiv.org/abs/2410.00324)|null|\n", "2410.02762": "|**2024-10-03**|**Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations**|1. University of California, <br>Berkeley|[2410.02762](http://arxiv.org/abs/2410.02762)|**[link](https://github.com/nickjiang2378/vl-interp)**|\n", "2410.02746": "|**2024-10-03**|**Contrastive Localized Language-Image Pre-Training**|1. Apple|[2410.02746](http://arxiv.org/abs/2410.02746)|null|\n", "2410.02730": "|**2024-10-03**|**DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects**|1. HKUST 2. Tencent AI Lab 3. <br>Robotics X 4. University of <br>Pennsylvania|[2410.02730](http://arxiv.org/abs/2410.02730)|null|\n", "2410.02729": "|**2024-10-03**|**Unified Multi-Modal Interleaved Document Representation for Information Retrieval**|1.KAIST 2.DeepAuto|[2410.02729](http://arxiv.org/abs/2410.02729)|null|\n", "2410.02681": "|**2024-10-03**|**Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models**|1.SouthernUniversityofSciencea <br>ndTechnology <br>2.UniversityofWisconsin-Madiso <br>n|[2410.02681](http://arxiv.org/abs/2410.02681)|null|\n", "2410.02615": "|**2024-10-03**|**LoGra-Med: Long Context Multi-Graph Alignment for Medical Vision-Language Model**|1. University of Stuttgart 2. <br>German Research Centre for <br>Artificial Intelligence 3. <br>Technical University of Munich <br>4. University Medical Center <br>G\u00f6ttingen|[2410.02615](http://arxiv.org/abs/2410.02615)|null|\n", "2410.02613": "|**2024-10-03**|**NL-Eye: Abductive NLI for Images**|1. Technion-IIT 2. Google <br>Research|[2410.02613](http://arxiv.org/abs/2410.02613)|null|\n", "2410.02492": "|**2024-10-03**|**DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking Based on LLM**|1. Chinese Academy of Sciences <br>2. University of Chinese <br>Academy of Sciences 3. Nanyang <br>Technological University 4. <br>CAS Center for Excellence in <br>Brain Science and Intelligence <br>Technology|[2410.02492](http://arxiv.org/abs/2410.02492)|null|\n", "2410.02193": "|**2024-10-03**|**Guiding Long-Horizon Task and Motion Planning with Vision Language Models**|1.Massachusetts Institute of <br>Technology 2.NVIDIA Research|[2410.02193](http://arxiv.org/abs/2410.02193)|null|\n", "2410.02052": "|**2024-10-02**|**Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning**|1.ColumbiaUniversity <br>2.MicrosoftResearch|[2410.02052](http://arxiv.org/abs/2410.02052)|null|\n", "2410.02049": "|**2024-10-02**|**Emo3D: Metric and Benchmarking Dataset for 3D Facial Expression Generation from Emotion Description**|1.SharifUniversityofTechnology <br>2.SharifUniversityofTechnology <br>3.SharifUniversityofTechnology <br>4.QatarComputingResearchInstit <br>ute|[2410.02049](http://arxiv.org/abs/2410.02049)|null|\n", "2410.02027": "|**2024-10-02**|**Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval**|1. Intelligent Systems Program <br>2. Department of Computer <br>Science, University of <br>Pittsburgh|[2410.02027](http://arxiv.org/abs/2410.02027)|null|\n", "2410.01971": "|**2024-10-02**|**Run-time Observation Interventions Make Vision-Language-Action Models More Visually Robust**|1. Princeton University|[2410.01971](http://arxiv.org/abs/2410.01971)|null|\n", "2410.01966": "|**2024-10-02**|**Enhancing Screen Time Identification in Children with a Multi-View Vision Language Model and Screen Time Tracker**|1. Stevens Institute of <br>Technology 2. Iowa State <br>University 3. Columbia <br>University in the City of New <br>York 4. Nokia Bell Labs|[2410.01966](http://arxiv.org/abs/2410.01966)|null|\n", "2410.01926": "|**2024-10-02**|**MARPLE: A Benchmark for Long-Horizon Inference**|1.StanfordUniversity|[2410.01926](http://arxiv.org/abs/2410.01926)|null|\n", "2410.01912": "|**2024-10-02**|**A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation**|1.PekingUniversity <br>2.AlibabaGroup <br>3.UniversityofWisconsin\u2013Madiso <br>n <br>4.BeijingInstituteofTechnology|[2410.01912](http://arxiv.org/abs/2410.01912)|**[link](https://github.com/chenllliang/dnd-transformer)**|\n", "2410.08211": "|**2024-10-10**|**LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts**|1. Amazon 2. Inria|[2410.08211](http://arxiv.org/abs/2410.08211)|null|\n", "2410.08182": "|**2024-10-10**|**MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models**| |[2410.08182](http://arxiv.org/abs/2410.08182)|null|\n", "2410.08172": "|**2024-10-10**|**On the Evaluation of Generative Robotic Simulations**| |[2410.08172](http://arxiv.org/abs/2410.08172)|null|\n", "2410.08119": "|**2024-10-10**|**Q-VLM: Post-training Quantization for Large Vision-Language Models**| |[2410.08119](http://arxiv.org/abs/2410.08119)|**[link](https://github.com/changyuanwang17/qvlm)**|\n", "2410.08021": "|**2024-10-10**|**OneRef: Unified One-tower Expression Grounding and Segmentation with Mask Referring Modeling**|1. MAIS, Institute of <br>Automation, Chinese Academy of <br>Sciences 2. Pengcheng <br>Laboratory 3. School of <br>Artificial Intelligence, <br>University of Chinese Academy <br>of Sciences 4. Harbin <br>Institute of Technology <br>(Shenzhen)|[2410.08021](http://arxiv.org/abs/2410.08021)|**[link](https://github.com/linhuixiao/oneref)**|\n", "2410.13861": "|**2024-10-17**|**PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**|1. CUHKMMLab 2. HKUMMLab 3. <br>SenseTime 4. <br>ShanghaiAILaboratory|[2410.13861](http://arxiv.org/abs/2410.13861)|**[link](https://github.com/rongyaofang/puma)**|\n", "2410.13860": "|**2024-10-17**|**VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding**|1. The Chinese University of <br>Hong Kong 2. Zhejiang <br>University 3. Shanghai AI <br>Laboratory 4. Centre for <br>Perceptual and Interactive <br>Intelligence|[2410.13860](http://arxiv.org/abs/2410.13860)|**[link](https://github.com/openrobotlab/vlm-grounder)**|\n", "2410.13859": "|**2024-10-17**|**$\u03b3-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models**|1. Technical University of <br>Denmark 2. OpenGVLab, Shanghai <br>AI Laboratory 3. Xiamen <br>University 4. National <br>University of Singapore|[2410.13859](http://arxiv.org/abs/2410.13859)|null|\n", "2410.13851": "|**2024-10-17**|**Differentiable Robot Rendering**|1.ColumbiaUniversity, <br>2.StanfordUniversity|[2410.13851](http://arxiv.org/abs/2410.13851)|null|\n", "2410.13823": "|**2024-10-17**|**Deep Generative Models Unveil Patterns in Medical Images Through Vision-Language Conditioning**|1. Imperial College London 2. <br>Imperial College London 3. <br>Imperial College London 4. <br>Imperial College London|[2410.13823](http://arxiv.org/abs/2410.13823)|**[link](https://github.com/junzhin/dgm-vlc)**|\n", "2410.17883": "|**2024-10-23**|**Lightweight Neural App Control**|1. Huawei Noah\u2019sArkLab 2. <br>University College London|[2410.17883](http://arxiv.org/abs/2410.17883)|null|\n", "2410.17856": "|**2024-10-23**|**ROCKET-1: Master Open-World Interaction with Visual-Temporal Context Prompting**|1.PKU, 2.UCLA, 3.BIGAI|[2410.17856](http://arxiv.org/abs/2410.17856)|null|\n", "2410.17827": "|**2024-10-23**|**RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models for Multi-label Chest X-ray Classification**|1. Universit\u00e0 degli Studi di <br>Firenze|[2410.17827](http://arxiv.org/abs/2410.17827)|null|\n", "2410.17809": "|**2024-10-23**|**An Intelligent Agentic System for Complex Image Restoration Problems**|1. <br>ShanghaiArtificialIntelligence <br>Laboratory 2. <br>ShanghaiJiaoTongUniversity 3. <br>TheUniversityofSydney 4. <br>TheChineseUniversityofHongKong|[2410.17809](http://arxiv.org/abs/2410.17809)|**[link](https://github.com/Kaiwen-Zhu/AgenticIR)**|\n", "2410.17779": "|**2024-10-23**|**ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language Tuning**|1. Beijing Institute of <br>Technology 2. The University <br>of Sydney 3. Sun Yat-sen <br>University 4. Wuhan University|[2410.17779](http://arxiv.org/abs/2410.17779)|**[link](https://github.com/hao840/adem-vl)**|\n", "2410.18857": "|**2024-10-24**|**Probabilistic Language-Image Pre-Training**|1. NAVER AI Lab|[2410.18857](http://arxiv.org/abs/2410.18857)|null|\n", "2410.18570": "|**2024-10-24**|**Zero-shot Object Navigation with Vision-Language Models Reasoning**|1. New York University Abu <br>Dhabi 2. New York University <br>3. China Academic of <br>Electronics and Information <br>Technology 4. Tsinghua <br>University|[2410.18570](http://arxiv.org/abs/2410.18570)|null|\n", "2410.18558": "|**2024-10-24**|**Infinity-MM: Scaling Multimodal Performance with Large-Scale and High-Quality Instruction Data**|1. BAAI 2. BJTU 3. BUPT 4. <br>ICT/CAS|[2410.18558](http://arxiv.org/abs/2410.18558)|null|\n", "2410.18537": "|**2024-10-24**|**Beyond Color and Lines: Zero-Shot Style-Specific Image Variations with Coordinated Semantics**|1. Northwest University|[2410.18537](http://arxiv.org/abs/2410.18537)|null|\n", "2410.18387": "|**2024-10-24**|**Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks**|1. The Hong Kong University of <br>Science and Technology 2. Sun <br>Yat-Sen Memorial Hospital, Sun <br>Yat-Sen University|[2410.18387](http://arxiv.org/abs/2410.18387)|null|\n", "2410.17772": "|**2024-10-23**|**Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models**|1. Karlsruhe Institute of <br>Technology 2. UC Berkeley|[2410.17772](http://arxiv.org/abs/2410.17772)|null|\n", "2410.17637": "|**2024-10-23**|**MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models**|1. SJTU 2. Shanghai AI <br>Laboratory 3. CUHK 4. <br>MThreads, Inc.|[2410.17637](http://arxiv.org/abs/2410.17637)|**[link](https://github.com/liuziyu77/mia-dpo)**|\n", "2410.17401": "|**2024-10-22**|**AdvWeb: Controllable Black-box Attacks on VLM-powered Web Agents**|1. University of Illinois <br>Urbana-Champaign 2. University <br>of Chicago 3. The Ohio State <br>University 4. University of <br>Science and Technology of <br>China|[2410.17401](http://arxiv.org/abs/2410.17401)|null|\n", "2410.17385": "|**2024-10-22**|**Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities**| |[2410.17385](http://arxiv.org/abs/2410.17385)|null|\n", "2410.17247": "|**2024-10-22**|**PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction**|1. USTC 2. Shanghai AI <br>Laboratory 3. CUHK|[2410.17247](http://arxiv.org/abs/2410.17247)|**[link](https://github.com/cooperx521/pyramiddrop)**|\n", "2410.17241": "|**2024-10-22**|**Frontiers in Intelligent Colonoscopy**|1. Nankai Institute of <br>Advanced Research 2. College <br>of Computer Science & VCIP, <br>Nankai University 3. School of <br>Computing, Australian National <br>University 4. Graduate School <br>of Science and Technology, <br>Keio University|[2410.17241](http://arxiv.org/abs/2410.17241)|**[link](https://github.com/ai4colonoscopy/intelliscope)**|\n", "2410.17152": "|**2024-10-22**|**Improving Pinterest Search Relevance Using Large Language Models**|1. Pinterest 2. Pinterest 3. <br>Pinterest 4. Pinterest|[2410.17152](http://arxiv.org/abs/2410.17152)|null|\n", "2410.17149": "|**2024-10-22**|**Are Visual-Language Models Effective in Action Recognition? A Comparative Study**|1. Inria Center at Universit\u00e9 <br>C\u00f4te d\u2019Azur|[2410.17149](http://arxiv.org/abs/2410.17149)|null|\n", "2410.16840": "|**2024-10-22**|**MPDS: A Movie Posters Dataset for Image Generation with Diffusion Model**|1. Nanjing University of <br>Science and Technology 2. <br>SeetaCloud|[2410.16840](http://arxiv.org/abs/2410.16840)|null|\n", "2410.16820": "|**2024-10-22**|**AttriPrompter: Auto-Prompting with Attribute Semantics for Zero-shot Nuclei Detection via Visual-Language Pre-trained Models**|1. Beihang University 2. <br>ByteDance Inc. 3. Zhejiang <br>University 4. Chinese Academy <br>of Medical Sciences|[2410.16820](http://arxiv.org/abs/2410.16820)|**[link](https://github.com/wuyongjiancode/attriprompter)**|\n"}}